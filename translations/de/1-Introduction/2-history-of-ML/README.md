<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b2d11df10030cacc41427a1fbc8bc8f1",
  "translation_date": "2025-09-03T21:52:04+00:00",
  "source_file": "1-Introduction/2-history-of-ML/README.md",
  "language_code": "de"
}
-->
# Geschichte des maschinellen Lernens

![Zusammenfassung der Geschichte des maschinellen Lernens in einer Sketchnote](../../../../translated_images/ml-history.a1bdfd4ce1f464d9a0502f38d355ffda384c95cd5278297a46c9a391b5053bc4.de.png)
> Sketchnote von [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Quiz vor der Vorlesung](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/3/)

---

[![ML für Anfänger - Geschichte des maschinellen Lernens](https://img.youtube.com/vi/N6wxM4wZ7V0/0.jpg)](https://youtu.be/N6wxM4wZ7V0 "ML für Anfänger - Geschichte des maschinellen Lernens")

> 🎥 Klicken Sie auf das Bild oben, um ein kurzes Video zu dieser Lektion anzusehen.

In dieser Lektion gehen wir die wichtigsten Meilensteine in der Geschichte des maschinellen Lernens und der künstlichen Intelligenz durch.

Die Geschichte der künstlichen Intelligenz (KI) als Forschungsfeld ist eng mit der Geschichte des maschinellen Lernens verbunden, da die Algorithmen und rechnerischen Fortschritte, die ML zugrunde liegen, die Entwicklung der KI vorangetrieben haben. Es ist wichtig zu bedenken, dass sich diese Bereiche zwar erst in den 1950er Jahren als eigenständige Forschungsgebiete herauskristallisierten, aber wichtige [algorithmische, statistische, mathematische, rechnerische und technische Entdeckungen](https://wikipedia.org/wiki/Timeline_of_machine_learning) bereits vorher gemacht wurden und sich mit dieser Ära überschnitten. Tatsächlich beschäftigen sich Menschen schon seit [Hunderten von Jahren](https://wikipedia.org/wiki/History_of_artificial_intelligence) mit diesen Fragen: Dieser Artikel beleuchtet die historischen intellektuellen Grundlagen der Idee einer „denkenden Maschine“.

---
## Bedeutende Entdeckungen

- 1763, 1812 [Bayes-Theorem](https://wikipedia.org/wiki/Bayes%27_theorem) und seine Vorgänger. Dieses Theorem und seine Anwendungen bilden die Grundlage der Inferenz und beschreiben die Wahrscheinlichkeit eines Ereignisses basierend auf vorherigem Wissen.
- 1805 [Methode der kleinsten Quadrate](https://wikipedia.org/wiki/Least_squares) von dem französischen Mathematiker Adrien-Marie Legendre. Diese Theorie, die Sie in unserer Regressionseinheit kennenlernen werden, hilft bei der Datenanpassung.
- 1913 [Markow-Ketten](https://wikipedia.org/wiki/Markov_chain), benannt nach dem russischen Mathematiker Andrey Markov, werden verwendet, um eine Abfolge möglicher Ereignisse basierend auf einem vorherigen Zustand zu beschreiben.
- 1957 [Perceptron](https://wikipedia.org/wiki/Perceptron) ist eine Art linearer Klassifikator, der von dem amerikanischen Psychologen Frank Rosenblatt erfunden wurde und die Grundlage für Fortschritte im Deep Learning bildet.

---

- 1967 [Nächster Nachbar](https://wikipedia.org/wiki/Nearest_neighbor) ist ein Algorithmus, der ursprünglich zur Routenplanung entwickelt wurde. Im ML-Kontext wird er zur Mustererkennung verwendet.
- 1970 [Backpropagation](https://wikipedia.org/wiki/Backpropagation) wird verwendet, um [Feedforward-Neuronale Netze](https://wikipedia.org/wiki/Feedforward_neural_network) zu trainieren.
- 1982 [Rekurrente Neuronale Netze](https://wikipedia.org/wiki/Recurrent_neural_network) sind künstliche neuronale Netze, die aus Feedforward-Netzen abgeleitet sind und zeitliche Graphen erstellen.

✅ Machen Sie ein wenig Recherche. Welche anderen Daten sind Ihrer Meinung nach entscheidend in der Geschichte des maschinellen Lernens und der KI?

---
## 1950: Maschinen, die denken

Alan Turing, eine wirklich bemerkenswerte Persönlichkeit, die [2019 von der Öffentlichkeit](https://wikipedia.org/wiki/Icons:_The_Greatest_Person_of_the_20th_Century) als der größte Wissenschaftler des 20. Jahrhunderts gewählt wurde, wird zugeschrieben, die Grundlage für das Konzept einer „denkenden Maschine“ gelegt zu haben. Er setzte sich mit Kritikern auseinander und suchte nach empirischen Beweisen für dieses Konzept, unter anderem durch die Entwicklung des [Turing-Tests](https://www.bbc.com/news/technology-18475646), den Sie in unseren NLP-Lektionen näher kennenlernen werden.

---
## 1956: Dartmouth Summer Research Project

„Das Dartmouth Summer Research Project zur künstlichen Intelligenz war ein wegweisendes Ereignis für die KI als Forschungsfeld“, und hier wurde der Begriff „künstliche Intelligenz“ geprägt ([Quelle](https://250.dartmouth.edu/highlights/artificial-intelligence-ai-coined-dartmouth)).

> Jeder Aspekt des Lernens oder eines anderen Merkmals der Intelligenz kann im Prinzip so genau beschrieben werden, dass eine Maschine dazu gebracht werden kann, ihn zu simulieren.

---

Der leitende Forscher, Mathematikprofessor John McCarthy, hoffte „auf der Grundlage der Vermutung vorzugehen, dass jeder Aspekt des Lernens oder eines anderen Merkmals der Intelligenz im Prinzip so genau beschrieben werden kann, dass eine Maschine dazu gebracht werden kann, ihn zu simulieren.“ Zu den Teilnehmern gehörte auch eine weitere bedeutende Persönlichkeit des Feldes, Marvin Minsky.

Der Workshop wird dafür anerkannt, mehrere Diskussionen angestoßen und gefördert zu haben, darunter „der Aufstieg symbolischer Methoden, Systeme, die sich auf begrenzte Domänen konzentrieren (frühe Expertensysteme), und deduktive Systeme versus induktive Systeme.“ ([Quelle](https://wikipedia.org/wiki/Dartmouth_workshop)).

---
## 1956 - 1974: „Die goldenen Jahre“

Von den 1950er Jahren bis Mitte der 1970er Jahre herrschte große Zuversicht, dass KI viele Probleme lösen könnte. 1967 erklärte Marvin Minsky selbstbewusst: „Innerhalb einer Generation ... wird das Problem der Schaffung von ‚künstlicher Intelligenz‘ im Wesentlichen gelöst sein.“ (Minsky, Marvin (1967), Computation: Finite and Infinite Machines, Englewood Cliffs, N.J.: Prentice-Hall)

Die Forschung zur Verarbeitung natürlicher Sprache blühte auf, Suchalgorithmen wurden verfeinert und leistungsfähiger gemacht, und das Konzept der „Mikrowelten“ wurde entwickelt, in denen einfache Aufgaben mit einfachen Sprachbefehlen ausgeführt werden konnten.

---

Die Forschung wurde von Regierungsbehörden gut finanziert, Fortschritte in der Berechnung und bei Algorithmen wurden erzielt, und Prototypen intelligenter Maschinen wurden gebaut. Einige dieser Maschinen umfassen:

* [Shakey der Roboter](https://wikipedia.org/wiki/Shakey_the_robot), der sich bewegen und entscheiden konnte, wie er Aufgaben „intelligent“ ausführt.

    ![Shakey, ein intelligenter Roboter](../../../../translated_images/shakey.4dc17819c447c05bf4b52f76da0bdd28817d056fdb906252ec20124dd4cfa55e.de.jpg)
    > Shakey im Jahr 1972

---

* Eliza, ein früher „Chatterbot“, konnte mit Menschen kommunizieren und als primitiver „Therapeut“ fungieren. Sie werden mehr über Eliza in den NLP-Lektionen erfahren.

    ![Eliza, ein Bot](../../../../translated_images/eliza.84397454cda9559bb5ec296b5b8fff067571c0cccc5405f9c1ab1c3f105c075c.de.png)
    > Eine Version von Eliza, ein Chatbot

---

* „Blocks World“ war ein Beispiel für eine Mikrowelt, in der Blöcke gestapelt und sortiert werden konnten und Experimente zur Entscheidungsfindung von Maschinen durchgeführt wurden. Fortschritte mit Bibliotheken wie [SHRDLU](https://wikipedia.org/wiki/SHRDLU) trieben die Sprachverarbeitung voran.

    [![Blocks World mit SHRDLU](https://img.youtube.com/vi/QAJz4YKUwqw/0.jpg)](https://www.youtube.com/watch?v=QAJz4YKUwqw "Blocks World mit SHRDLU")

    > 🎥 Klicken Sie auf das Bild oben für ein Video: Blocks World mit SHRDLU

---
## 1974 - 1980: „AI Winter“

Mitte der 1970er Jahre wurde klar, dass die Komplexität der Schaffung „intelligenter Maschinen“ unterschätzt und ihr Versprechen angesichts der verfügbaren Rechenleistung überbewertet worden war. Die Finanzierung versiegte und das Vertrauen in das Feld nahm ab. Einige Probleme, die das Vertrauen beeinträchtigten, waren:
---
- **Einschränkungen**. Die Rechenleistung war zu begrenzt.
- **Kombinatorische Explosion**. Die Anzahl der zu trainierenden Parameter wuchs exponentiell, je mehr von Computern verlangt wurde, ohne dass sich die Rechenleistung und -fähigkeit parallel weiterentwickelten.
- **Mangel an Daten**. Es gab einen Mangel an Daten, der den Prozess des Testens, Entwickelns und Verfeinerns von Algorithmen behinderte.
- **Stellen wir die richtigen Fragen?**. Die gestellten Fragen selbst wurden infrage gestellt. Forscher sahen sich Kritik an ihren Ansätzen gegenüber:
  - Turing-Tests wurden unter anderem durch die „Chinese Room Theory“ infrage gestellt, die besagt, dass „das Programmieren eines digitalen Computers ihn zwar so erscheinen lassen kann, als würde er Sprache verstehen, aber kein echtes Verständnis erzeugen könnte.“ ([Quelle](https://plato.stanford.edu/entries/chinese-room/))
  - Die Ethik der Einführung künstlicher Intelligenzen wie des „Therapeuten“ ELIZA in die Gesellschaft wurde herausgefordert.

---

Gleichzeitig begannen sich verschiedene Schulen der KI-Forschung zu bilden. Es entstand eine Dichotomie zwischen ["Scruffy" vs. "Neat AI"](https://wikipedia.org/wiki/Neats_and_scruffies)-Praktiken. _Scruffy_-Labore optimierten Programme stundenlang, bis sie die gewünschten Ergebnisse erzielten. _Neat_-Labore „konzentrierten sich auf Logik und formale Problemlösung“. ELIZA und SHRDLU waren bekannte _Scruffy_-Systeme. In den 1980er Jahren, als die Nachfrage nach reproduzierbaren ML-Systemen aufkam, setzte sich der _Neat_-Ansatz allmählich durch, da seine Ergebnisse besser erklärbar sind.

---
## 1980er Jahre: Expertensysteme

Mit dem Wachstum des Feldes wurde sein Nutzen für Unternehmen deutlicher, und in den 1980er Jahren verbreiteten sich „Expertensysteme“. „Expertensysteme gehörten zu den ersten wirklich erfolgreichen Formen von künstlicher Intelligenz (KI)-Software.“ ([Quelle](https://wikipedia.org/wiki/Expert_system)).

Diese Art von System ist tatsächlich _hybrid_ und besteht teilweise aus einer Regel-Engine, die Geschäftsanforderungen definiert, und einer Inferenz-Engine, die das Regelwerk nutzt, um neue Fakten abzuleiten.

In dieser Ära wurde auch den neuronalen Netzen zunehmend Aufmerksamkeit geschenkt.

---
## 1987 - 1993: AI „Chill“

Die Verbreitung spezialisierter Expertensystem-Hardware hatte den unglücklichen Effekt, zu spezialisiert zu werden. Der Aufstieg von Personal Computern konkurrierte mit diesen großen, spezialisierten, zentralisierten Systemen. Die Demokratisierung des Rechnens hatte begonnen und ebnete schließlich den Weg für die moderne Explosion von Big Data.

---
## 1993 - 2011

Diese Epoche markierte eine neue Ära für ML und KI, um einige der Probleme zu lösen, die zuvor durch den Mangel an Daten und Rechenleistung verursacht worden waren. Die Menge an Daten begann rapide zu wachsen und wurde zunehmend verfügbar, sowohl zum Guten als auch zum Schlechten, insbesondere mit der Einführung des Smartphones um 2007. Die Rechenleistung nahm exponentiell zu, und Algorithmen entwickelten sich parallel dazu weiter. Das Feld begann, Reife zu erlangen, da die ungebundenen Tage der Vergangenheit sich zu einer echten Disziplin formten.

---
## Heute

Heute berühren maschinelles Lernen und KI fast jeden Teil unseres Lebens. Diese Ära erfordert ein sorgfältiges Verständnis der Risiken und potenziellen Auswirkungen dieser Algorithmen auf das menschliche Leben. Wie Brad Smith von Microsoft erklärt hat: „Informationstechnologie wirft Fragen auf, die das Herzstück grundlegender Menschenrechtsfragen wie Privatsphäre und Meinungsfreiheit berühren. Diese Fragen erhöhen die Verantwortung für Technologieunternehmen, die diese Produkte entwickeln. Aus unserer Sicht erfordern sie auch eine durchdachte staatliche Regulierung und die Entwicklung von Normen für akzeptable Anwendungen.“ ([Quelle](https://www.technologyreview.com/2019/12/18/102365/the-future-of-ais-impact-on-society/)).

---

Es bleibt abzuwarten, was die Zukunft bringt, aber es ist wichtig, diese Computersysteme sowie die Software und Algorithmen, die sie ausführen, zu verstehen. Wir hoffen, dass dieses Curriculum Ihnen hilft, ein besseres Verständnis zu erlangen, damit Sie selbst entscheiden können.

[![Die Geschichte des Deep Learning](https://img.youtube.com/vi/mTtDfKgLm54/0.jpg)](https://www.youtube.com/watch?v=mTtDfKgLm54 "Die Geschichte des Deep Learning")
> 🎥 Klicken Sie auf das Bild oben für ein Video: Yann LeCun diskutiert die Geschichte des Deep Learning in diesem Vortrag

---
## 🚀 Herausforderung

Tauchen Sie in einen dieser historischen Momente ein und erfahren Sie mehr über die Menschen dahinter. Es gibt faszinierende Persönlichkeiten, und keine wissenschaftliche Entdeckung wurde jemals in einem kulturellen Vakuum gemacht. Was entdecken Sie?

## [Quiz nach der Vorlesung](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/4/)

---
## Überprüfung & Selbststudium

Hier sind einige Dinge, die Sie sich ansehen und anhören können:

[Dieser Podcast, in dem Amy Boyd die Entwicklung der KI diskutiert](http://runasradio.com/Shows/Show/739)

[![Die Geschichte der KI von Amy Boyd](https://img.youtube.com/vi/EJt3_bFYKss/0.jpg)](https://www.youtube.com/watch?v=EJt3_bFYKss "Die Geschichte der KI von Amy Boyd")

---

## Aufgabe

[Erstellen Sie eine Zeitleiste](assignment.md)

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.