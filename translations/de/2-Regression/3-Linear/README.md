<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2f88fbc741d792890ff2f1430fe0dae0",
  "translation_date": "2025-09-03T21:38:33+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "de"
}
-->
# Erstellen eines Regressionsmodells mit Scikit-learn: Regression auf vier Arten

![Infografik zu linearer vs. polynomialer Regression](../../../../translated_images/linear-polynomial.5523c7cb6576ccab0fecbd0e3505986eb2d191d9378e785f82befcf3a578a6e7.de.png)
> Infografik von [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Quiz vor der Vorlesung](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/13/)

> ### [Diese Lektion ist auch in R verf√ºgbar!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Einf√ºhrung 

Bisher hast du untersucht, was Regression ist, anhand von Beispieldaten aus dem K√ºrbispreis-Datensatz, den wir in dieser Lektion verwenden werden. Du hast ihn auch mit Matplotlib visualisiert.

Jetzt bist du bereit, tiefer in die Regression f√ºr maschinelles Lernen einzutauchen. W√§hrend die Visualisierung hilft, Daten zu verstehen, liegt die wahre St√§rke des maschinellen Lernens im _Trainieren von Modellen_. Modelle werden mit historischen Daten trainiert, um automatisch Datenabh√§ngigkeiten zu erfassen, und sie erm√∂glichen es, Ergebnisse f√ºr neue Daten vorherzusagen, die das Modell zuvor nicht gesehen hat.

In dieser Lektion wirst du mehr √ºber zwei Arten von Regression lernen: _einfache lineare Regression_ und _polynomiale Regression_, zusammen mit einigen mathematischen Grundlagen dieser Techniken. Diese Modelle erm√∂glichen es uns, K√ºrbispreise basierend auf verschiedenen Eingabedaten vorherzusagen.

[![ML f√ºr Anf√§nger - Verst√§ndnis der linearen Regression](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML f√ºr Anf√§nger - Verst√§ndnis der linearen Regression")

> üé• Klicke auf das Bild oben f√ºr eine kurze Video√ºbersicht zur linearen Regression.

> Im gesamten Lehrplan gehen wir von minimalen Mathematikkenntnissen aus und versuchen, das Thema f√ºr Studierende aus anderen Bereichen zug√§nglich zu machen. Achte auf Notizen, üßÆ Hinweise, Diagramme und andere Lernhilfen, die das Verst√§ndnis erleichtern.

### Voraussetzungen

Du solltest inzwischen mit der Struktur der K√ºrbisdaten vertraut sein, die wir untersuchen. Du findest sie vorab geladen und bereinigt in der Datei _notebook.ipynb_ dieser Lektion. In der Datei wird der K√ºrbispreis pro Scheffel in einem neuen DataFrame angezeigt. Stelle sicher, dass du diese Notebooks in Visual Studio Code ausf√ºhren kannst.

### Vorbereitung

Zur Erinnerung: Du l√§dst diese Daten, um Fragen dazu zu stellen.

- Wann ist die beste Zeit, K√ºrbisse zu kaufen? 
- Welchen Preis kann ich f√ºr eine Kiste Miniaturk√ºrbisse erwarten?
- Sollte ich sie in halben Scheffelk√∂rben oder in 1 1/9 Scheffelkisten kaufen?
Lass uns weiter in diese Daten eintauchen.

In der vorherigen Lektion hast du einen Pandas-DataFrame erstellt und ihn mit einem Teil des urspr√ºnglichen Datensatzes gef√ºllt, wobei du die Preise pro Scheffel standardisiert hast. Dadurch konntest du jedoch nur etwa 400 Datenpunkte sammeln, und das nur f√ºr die Herbstmonate.

Schau dir die Daten an, die wir in dem begleitenden Notebook dieser Lektion vorab geladen haben. Die Daten sind vorab geladen, und ein erster Streudiagramm wurde erstellt, um Monatsdaten zu zeigen. Vielleicht k√∂nnen wir durch eine gr√ºndlichere Bereinigung der Daten noch mehr Details √ºber die Natur der Daten erhalten.

## Eine lineare Regressionslinie

Wie du in Lektion 1 gelernt hast, besteht das Ziel einer linearen Regression darin, eine Linie zu zeichnen, um:

- **Beziehungen zwischen Variablen zu zeigen**. Die Beziehung zwischen Variablen darzustellen.
- **Vorhersagen zu treffen**. Genaue Vorhersagen dar√ºber zu machen, wo ein neuer Datenpunkt im Verh√§ltnis zu dieser Linie liegen w√ºrde.

Typisch f√ºr die **Methode der kleinsten Quadrate** ist es, diese Art von Linie zu zeichnen. Der Begriff "kleinste Quadrate" bedeutet, dass alle Datenpunkte um die Regressionslinie quadriert und dann addiert werden. Idealerweise ist diese Summe so klein wie m√∂glich, da wir eine geringe Fehleranzahl oder `kleinste Quadrate` anstreben.

Wir tun dies, da wir eine Linie modellieren m√∂chten, die die geringste kumulative Entfernung von allen unseren Datenpunkten hat. Wir quadrieren die Terme vor dem Addieren, da uns die Gr√∂√üe der Abweichung wichtiger ist als ihre Richtung.

> **üßÆ Zeig mir die Mathematik** 
> 
> Diese Linie, die als _Best-Fit-Linie_ bezeichnet wird, kann durch [eine Gleichung](https://en.wikipedia.org/wiki/Simple_linear_regression) ausgedr√ºckt werden: 
> 
> ```
> Y = a + bX
> ```
>
> `X` ist die 'erkl√§rende Variable'. `Y` ist die 'abh√§ngige Variable'. Die Steigung der Linie ist `b`, und `a` ist der y-Achsenabschnitt, der den Wert von `Y` angibt, wenn `X = 0`. 
>
>![Berechnung der Steigung](../../../../translated_images/slope.f3c9d5910ddbfcf9096eb5564254ba22c9a32d7acd7694cab905d29ad8261db3.de.png)
>
> Zuerst berechnen wir die Steigung `b`. Infografik von [Jen Looper](https://twitter.com/jenlooper)
>
> Mit Bezug auf die urspr√ºngliche Frage zu den K√ºrbisdaten: "Vorhersage des Preises eines K√ºrbisses pro Scheffel nach Monat", w√ºrde `X` den Preis und `Y` den Verkaufsmonat darstellen. 
>
>![Vervollst√§ndigung der Gleichung](../../../../translated_images/calculation.a209813050a1ddb141cdc4bc56f3af31e67157ed499e16a2ecf9837542704c94.de.png)
>
> Berechnung des Wertes von Y. Wenn du etwa 4 $ zahlst, muss es April sein! Infografik von [Jen Looper](https://twitter.com/jenlooper)
>
> Die Mathematik, die die Linie berechnet, muss die Steigung der Linie zeigen, die auch vom Achsenabschnitt abh√§ngt, oder wo `Y` liegt, wenn `X = 0`.
>
> Du kannst die Methode zur Berechnung dieser Werte auf der Website [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) beobachten. Besuche auch [diesen Rechner f√ºr die Methode der kleinsten Quadrate](https://www.mathsisfun.com/data/least-squares-calculator.html), um zu sehen, wie die Werte die Linie beeinflussen.

## Korrelation

Ein weiterer Begriff, den du verstehen solltest, ist der **Korrelationskoeffizient** zwischen den gegebenen X- und Y-Variablen. Mit einem Streudiagramm kannst du diesen Koeffizienten schnell visualisieren. Ein Diagramm mit Datenpunkten, die in einer ordentlichen Linie verstreut sind, hat eine hohe Korrelation, w√§hrend ein Diagramm mit Datenpunkten, die √ºberall zwischen X und Y verstreut sind, eine niedrige Korrelation hat.

Ein gutes lineares Regressionsmodell ist eines, das eine hohe (n√§her an 1 als an 0) Korrelation aufweist, basierend auf der Methode der kleinsten Quadrate mit einer Regressionslinie.

‚úÖ F√ºhre das begleitende Notebook dieser Lektion aus und sieh dir das Streudiagramm von Monat zu Preis an. Scheint die Datenassoziation zwischen Monat und Preis f√ºr K√ºrbisverk√§ufe laut deiner visuellen Interpretation des Streudiagramms eine hohe oder niedrige Korrelation zu haben? √Ñndert sich das, wenn du eine feinere Messung anstelle von `Monat` verwendest, z. B. *Tag des Jahres* (d. h. Anzahl der Tage seit Jahresbeginn)?

Im folgenden Code nehmen wir an, dass wir die Daten bereinigt haben und einen DataFrame namens `new_pumpkins` erhalten haben, √§hnlich dem folgenden:

ID | Monat | TagDesJahres | Sorte | Stadt | Verpackung | Niedriger Preis | Hoher Preis | Preis
---|-------|--------------|-------|-------|------------|-----------------|-------------|------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> Der Code zur Bereinigung der Daten ist verf√ºgbar in [`notebook.ipynb`](notebook.ipynb). Wir haben die gleichen Bereinigungsschritte wie in der vorherigen Lektion durchgef√ºhrt und die Spalte `TagDesJahres` mit folgendem Ausdruck berechnet: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Jetzt, da du die Mathematik hinter der linearen Regression verstehst, lass uns ein Regressionsmodell erstellen, um zu sehen, ob wir vorhersagen k√∂nnen, welches K√ºrbispaket die besten K√ºrbispreise hat. Jemand, der K√ºrbisse f√ºr einen Feiertags-K√ºrbisstand kauft, k√∂nnte diese Informationen ben√∂tigen, um seine Eink√§ufe von K√ºrbispaketen f√ºr den Stand zu optimieren.

## Suche nach Korrelation

[![ML f√ºr Anf√§nger - Suche nach Korrelation: Der Schl√ºssel zur linearen Regression](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML f√ºr Anf√§nger - Suche nach Korrelation: Der Schl√ºssel zur linearen Regression")

> üé• Klicke auf das Bild oben f√ºr eine kurze Video√ºbersicht zur Korrelation.

Aus der vorherigen Lektion hast du wahrscheinlich gesehen, dass der Durchschnittspreis f√ºr verschiedene Monate wie folgt aussieht:

<img alt="Durchschnittspreis nach Monat" src="../2-Data/images/barchart.png" width="50%"/>

Dies deutet darauf hin, dass es eine gewisse Korrelation geben sollte, und wir k√∂nnen versuchen, ein lineares Regressionsmodell zu trainieren, um die Beziehung zwischen `Monat` und `Preis` oder zwischen `TagDesJahres` und `Preis` vorherzusagen. Hier ist das Streudiagramm, das die letztere Beziehung zeigt:

<img alt="Streudiagramm von Preis vs. Tag des Jahres" src="images/scatter-dayofyear.png" width="50%" /> 

Lass uns sehen, ob es eine Korrelation gibt, indem wir die Funktion `corr` verwenden:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Es sieht so aus, als ob die Korrelation ziemlich gering ist, -0.15 f√ºr `Monat` und -0.17 f√ºr `TagDesJahres`, aber es k√∂nnte eine andere wichtige Beziehung geben. Es scheint, dass es verschiedene Preiscluster gibt, die mit verschiedenen K√ºrbissorten korrespondieren. Um diese Hypothese zu best√§tigen, lass uns jede K√ºrbiskategorie mit einer anderen Farbe darstellen. Indem wir einen `ax`-Parameter an die `scatter`-Plot-Funktion √ºbergeben, k√∂nnen wir alle Punkte im selben Diagramm darstellen:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Streudiagramm von Preis vs. Tag des Jahres" src="images/scatter-dayofyear-color.png" width="50%" /> 

Unsere Untersuchung legt nahe, dass die Sorte einen gr√∂√üeren Einfluss auf den Gesamtpreis hat als das tats√§chliche Verkaufsdatum. Wir k√∂nnen dies mit einem Balkendiagramm sehen:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Balkendiagramm von Preis vs. Sorte" src="images/price-by-variety.png" width="50%" /> 

Lass uns f√ºr den Moment nur auf eine K√ºrbissorte, den 'Pie Type', fokussieren und sehen, welchen Einfluss das Datum auf den Preis hat:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Streudiagramm von Preis vs. Tag des Jahres" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Wenn wir jetzt die Korrelation zwischen `Preis` und `TagDesJahres` mit der Funktion `corr` berechnen, erhalten wir etwa `-0.27` - was bedeutet, dass das Trainieren eines Vorhersagemodells sinnvoll ist.

> Bevor wir ein lineares Regressionsmodell trainieren, ist es wichtig sicherzustellen, dass unsere Daten sauber sind. Lineare Regression funktioniert nicht gut mit fehlenden Werten, daher macht es Sinn, alle leeren Zellen zu entfernen:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Eine andere Herangehensweise w√§re, diese leeren Werte mit den Mittelwerten der entsprechenden Spalte zu f√ºllen.

## Einfache lineare Regression

[![ML f√ºr Anf√§nger - Lineare und polynomiale Regression mit Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML f√ºr Anf√§nger - Lineare und polynomiale Regression mit Scikit-learn")

> üé• Klicke auf das Bild oben f√ºr eine kurze Video√ºbersicht zur linearen und polynomialen Regression.

Um unser lineares Regressionsmodell zu trainieren, verwenden wir die **Scikit-learn**-Bibliothek.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Wir beginnen damit, Eingabewerte (Features) und die erwartete Ausgabe (Label) in separate numpy-Arrays zu trennen:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Beachte, dass wir `reshape` auf die Eingabedaten anwenden mussten, damit das Paket f√ºr lineare Regression sie korrekt versteht. Lineare Regression erwartet ein 2D-Array als Eingabe, wobei jede Zeile des Arrays einem Vektor von Eingabefeatures entspricht. In unserem Fall, da wir nur eine Eingabe haben, ben√∂tigen wir ein Array mit der Form N√ó1, wobei N die Datensatzgr√∂√üe ist.

Dann m√ºssen wir die Daten in Trainings- und Testdatens√§tze aufteilen, damit wir unser Modell nach dem Training validieren k√∂nnen:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Das eigentliche Training des linearen Regressionsmodells dauert nur zwei Codezeilen. Wir definieren das `LinearRegression`-Objekt und passen es mit der Methode `fit` an unsere Daten an:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

Das `LinearRegression`-Objekt enth√§lt nach dem `fit`-Vorgang alle Koeffizienten der Regression, die √ºber die Eigenschaft `.coef_` abgerufen werden k√∂nnen. In unserem Fall gibt es nur einen Koeffizienten, der etwa `-0.017` sein sollte. Das bedeutet, dass die Preise mit der Zeit leicht sinken, aber nicht zu stark, etwa um 2 Cent pro Tag. Wir k√∂nnen auch den Schnittpunkt der Regression mit der Y-Achse √ºber `lin_reg.intercept_` abrufen - er wird in unserem Fall etwa `21` sein, was den Preis zu Jahresbeginn angibt.

Um zu sehen, wie genau unser Modell ist, k√∂nnen wir die Preise auf einem Testdatensatz vorhersagen und dann messen, wie nah unsere Vorhersagen an den erwarteten Werten liegen. Dies kann mit der Mean-Square-Error (MSE)-Metrik erfolgen, die den Mittelwert aller quadrierten Unterschiede zwischen erwartetem und vorhergesagtem Wert darstellt.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```
Unser Fehler scheint sich auf zwei Punkte zu konzentrieren, was etwa 17 % entspricht. Nicht besonders gut. Ein weiterer Indikator f√ºr die Modellqualit√§t ist der **Bestimmtheitskoeffizient**, der wie folgt berechnet werden kann:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Wenn der Wert 0 ist, bedeutet das, dass das Modell die Eingabedaten nicht ber√ºcksichtigt und als *schlechtester linearer Pr√§diktor* agiert, der einfach den Mittelwert des Ergebnisses darstellt. Der Wert 1 bedeutet, dass wir alle erwarteten Ausgaben perfekt vorhersagen k√∂nnen. In unserem Fall liegt der Koeffizient bei etwa 0,06, was ziemlich niedrig ist.

Wir k√∂nnen auch die Testdaten zusammen mit der Regressionslinie darstellen, um besser zu sehen, wie die Regression in unserem Fall funktioniert:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Lineare Regression" src="images/linear-results.png" width="50%" />

## Polynomiale Regression

Eine andere Art der linearen Regression ist die polynomiale Regression. W√§hrend es manchmal eine lineare Beziehung zwischen Variablen gibt ‚Äì je gr√∂√üer der K√ºrbis im Volumen, desto h√∂her der Preis ‚Äì k√∂nnen diese Beziehungen manchmal nicht als Ebene oder gerade Linie dargestellt werden.

‚úÖ Hier sind [einige weitere Beispiele](https://online.stat.psu.edu/stat501/lesson/9/9.8) f√ºr Daten, die polynomiale Regression verwenden k√∂nnten.

Betrachten Sie erneut die Beziehung zwischen Datum und Preis. Sieht dieses Streudiagramm so aus, als sollte es unbedingt durch eine gerade Linie analysiert werden? K√∂nnen Preise nicht schwanken? In diesem Fall k√∂nnen Sie polynomiale Regression ausprobieren.

‚úÖ Polynome sind mathematische Ausdr√ºcke, die aus einer oder mehreren Variablen und Koeffizienten bestehen k√∂nnen.

Die polynomiale Regression erstellt eine gekr√ºmmte Linie, um nichtlineare Daten besser anzupassen. In unserem Fall sollten wir, wenn wir eine quadrierte `DayOfYear`-Variable in die Eingabedaten aufnehmen, unsere Daten mit einer parabolischen Kurve anpassen k√∂nnen, die zu einem bestimmten Zeitpunkt im Jahr ein Minimum erreicht.

Scikit-learn enth√§lt eine hilfreiche [Pipeline-API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline), um verschiedene Schritte der Datenverarbeitung zu kombinieren. Eine **Pipeline** ist eine Kette von **Sch√§tzern**. In unserem Fall erstellen wir eine Pipeline, die zuerst polynomiale Merkmale zu unserem Modell hinzuf√ºgt und dann die Regression trainiert:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

Die Verwendung von `PolynomialFeatures(2)` bedeutet, dass wir alle Polynome zweiten Grades aus den Eingabedaten einbeziehen. In unserem Fall bedeutet das einfach `DayOfYear`<sup>2</sup>, aber bei zwei Eingabevariablen X und Y f√ºgt dies X<sup>2</sup>, XY und Y<sup>2</sup> hinzu. Wir k√∂nnen auch Polynome h√∂heren Grades verwenden, wenn wir m√∂chten.

Pipelines k√∂nnen genauso verwendet werden wie das urspr√ºngliche `LinearRegression`-Objekt, d. h. wir k√∂nnen die Pipeline `fit`ten und dann `predict` verwenden, um die Vorhersageergebnisse zu erhalten. Hier ist das Diagramm, das Testdaten und die Ann√§herungskurve zeigt:

<img alt="Polynomiale Regression" src="images/poly-results.png" width="50%" />

Mit der polynomialen Regression k√∂nnen wir einen etwas niedrigeren MSE und einen h√∂heren Bestimmtheitskoeffizienten erzielen, aber nicht signifikant. Wir m√ºssen andere Merkmale ber√ºcksichtigen!

> Sie k√∂nnen sehen, dass die minimalen K√ºrbispreise irgendwo um Halloween beobachtet werden. Wie k√∂nnen Sie das erkl√§ren?

üéÉ Herzlichen Gl√ºckwunsch, Sie haben gerade ein Modell erstellt, das helfen kann, den Preis von K√ºrbissen f√ºr Kuchen vorherzusagen. Sie k√∂nnen wahrscheinlich dasselbe Verfahren f√ºr alle K√ºrbissorten wiederholen, aber das w√§re m√ºhsam. Lernen wir jetzt, wie man K√ºrbissorten in unser Modell einbezieht!

## Kategorische Merkmale

In der idealen Welt m√∂chten wir in der Lage sein, Preise f√ºr verschiedene K√ºrbissorten mit demselben Modell vorherzusagen. Die Spalte `Variety` unterscheidet sich jedoch etwas von Spalten wie `Month`, da sie nicht-numerische Werte enth√§lt. Solche Spalten werden als **kategorisch** bezeichnet.

[![ML f√ºr Anf√§nger - Kategorische Merkmalsvorhersagen mit linearer Regression](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML f√ºr Anf√§nger - Kategorische Merkmalsvorhersagen mit linearer Regression")

> üé• Klicken Sie auf das Bild oben f√ºr eine kurze Video√ºbersicht zur Verwendung kategorischer Merkmale.

Hier k√∂nnen Sie sehen, wie der Durchschnittspreis von der Sorte abh√§ngt:

<img alt="Durchschnittspreis nach Sorte" src="images/price-by-variety.png" width="50%" />

Um die Sorte zu ber√ºcksichtigen, m√ºssen wir sie zuerst in numerische Form umwandeln, oder **codieren**. Es gibt mehrere M√∂glichkeiten, dies zu tun:

* Einfache **numerische Codierung** erstellt eine Tabelle mit verschiedenen Sorten und ersetzt dann den Sortennamen durch einen Index in dieser Tabelle. Dies ist keine gute Idee f√ºr die lineare Regression, da die lineare Regression den tats√§chlichen numerischen Wert des Index nimmt und ihn mit einem Koeffizienten multipliziert, um ihn zum Ergebnis hinzuzuf√ºgen. In unserem Fall ist die Beziehung zwischen der Indexnummer und dem Preis eindeutig nicht linear, selbst wenn wir sicherstellen, dass die Indizes in einer bestimmten Reihenfolge angeordnet sind.
* **One-hot-Codierung** ersetzt die Spalte `Variety` durch 4 verschiedene Spalten, eine f√ºr jede Sorte. Jede Spalte enth√§lt `1`, wenn die entsprechende Zeile einer bestimmten Sorte entspricht, und `0` andernfalls. Das bedeutet, dass es in der linearen Regression vier Koeffizienten gibt, einen f√ºr jede K√ºrbissorte, die f√ºr den "Startpreis" (oder eher "Zusatzpreis") f√ºr diese bestimmte Sorte verantwortlich sind.

Der folgende Code zeigt, wie wir eine Sorte one-hot codieren k√∂nnen:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Um die lineare Regression mit einer one-hot codierten Sorte als Eingabe zu trainieren, m√ºssen wir nur die `X`- und `y`-Daten korrekt initialisieren:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Der Rest des Codes ist derselbe wie der, den wir oben verwendet haben, um die lineare Regression zu trainieren. Wenn Sie es ausprobieren, werden Sie sehen, dass der mittlere quadratische Fehler ungef√§hr gleich bleibt, aber wir erhalten einen viel h√∂heren Bestimmtheitskoeffizienten (~77 %). Um noch genauere Vorhersagen zu erhalten, k√∂nnen wir mehr kategorische Merkmale sowie numerische Merkmale wie `Month` oder `DayOfYear` ber√ºcksichtigen. Um ein gro√ües Array von Merkmalen zu erhalten, k√∂nnen wir `join` verwenden:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Hier ber√ºcksichtigen wir auch `City` und `Package`-Typ, was uns einen MSE von 2,84 (10 %) und einen Bestimmtheitskoeffizienten von 0,94 gibt!

## Alles zusammenf√ºhren

Um das beste Modell zu erstellen, k√∂nnen wir kombinierte (one-hot codierte kategorische + numerische) Daten aus dem obigen Beispiel zusammen mit polynomialer Regression verwenden. Hier ist der vollst√§ndige Code zu Ihrer Bequemlichkeit:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Dies sollte uns den besten Bestimmtheitskoeffizienten von fast 97 % und MSE=2,23 (~8 % Vorhersagefehler) geben.

| Modell | MSE | Bestimmtheitskoeffizient |
|-------|-----|---------------------------|
| `DayOfYear` Linear | 2,77 (17,2 %) | 0,07 |
| `DayOfYear` Polynomial | 2,73 (17,0 %) | 0,08 |
| `Variety` Linear | 5,24 (19,7 %) | 0,77 |
| Alle Merkmale Linear | 2,84 (10,5 %) | 0,94 |
| Alle Merkmale Polynomial | 2,23 (8,25 %) | 0,97 |

üèÜ Gut gemacht! Sie haben in einer Lektion vier Regressionsmodelle erstellt und die Modellqualit√§t auf 97 % verbessert. Im letzten Abschnitt zur Regression lernen Sie die logistische Regression kennen, um Kategorien zu bestimmen.

---
## üöÄ Herausforderung

Testen Sie mehrere verschiedene Variablen in diesem Notebook, um zu sehen, wie die Korrelation mit der Modellgenauigkeit zusammenh√§ngt.

## [Quiz nach der Vorlesung](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/14/)

## √úberpr√ºfung & Selbststudium

In dieser Lektion haben wir √ºber lineare Regression gelernt. Es gibt andere wichtige Arten der Regression. Lesen Sie √ºber Stepwise-, Ridge-, Lasso- und Elasticnet-Techniken. Ein guter Kurs, um mehr zu lernen, ist der [Stanford Statistical Learning Kurs](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## Aufgabe

[Erstellen Sie ein Modell](assignment.md)

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.