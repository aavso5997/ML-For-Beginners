<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "085d571097d201810720df4cd379f8c2",
  "translation_date": "2025-09-03T23:11:20+00:00",
  "source_file": "5-Clustering/2-K-Means/README.md",
  "language_code": "es"
}
-->
# Agrupamiento K-Means

## [Cuestionario previo a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/29/)

En esta lecci√≥n, aprender√°s c√≥mo crear agrupaciones utilizando Scikit-learn y el conjunto de datos de m√∫sica nigeriana que importaste anteriormente. Cubriremos los conceptos b√°sicos de K-Means para el agrupamiento. Ten en cuenta que, como aprendiste en la lecci√≥n anterior, hay muchas formas de trabajar con agrupaciones y el m√©todo que utilices depende de tus datos. Probaremos K-Means ya que es la t√©cnica de agrupamiento m√°s com√∫n. ¬°Comencemos!

T√©rminos que aprender√°s:

- Puntuaci√≥n de silueta
- M√©todo del codo
- Inercia
- Varianza

## Introducci√≥n

[El agrupamiento K-Means](https://wikipedia.org/wiki/K-means_clustering) es un m√©todo derivado del √°mbito del procesamiento de se√±ales. Se utiliza para dividir y particionar grupos de datos en 'k' agrupaciones utilizando una serie de observaciones. Cada observaci√≥n trabaja para agrupar un punto de datos dado lo m√°s cerca posible de su 'media' m√°s cercana, o el punto central de una agrupaci√≥n.

Las agrupaciones pueden visualizarse como [diagramas de Voronoi](https://wikipedia.org/wiki/Voronoi_diagram), que incluyen un punto (o 'semilla') y su regi√≥n correspondiente.

![diagrama de voronoi](../../../../translated_images/voronoi.1dc1613fb0439b9564615eca8df47a4bcd1ce06217e7e72325d2406ef2180795.es.png)

> Infograf√≠a por [Jen Looper](https://twitter.com/jenlooper)

El proceso de agrupamiento K-Means [se ejecuta en tres pasos](https://scikit-learn.org/stable/modules/clustering.html#k-means):

1. El algoritmo selecciona un n√∫mero k de puntos centrales mediante muestreo del conjunto de datos. Despu√©s de esto, realiza un bucle:
    1. Asigna cada muestra al centroide m√°s cercano.
    2. Crea nuevos centroides tomando el valor promedio de todas las muestras asignadas a los centroides anteriores.
    3. Luego, calcula la diferencia entre los nuevos y los antiguos centroides y repite hasta que los centroides se estabilicen.

Una desventaja de usar K-Means es que necesitas establecer 'k', es decir, el n√∫mero de centroides. Afortunadamente, el 'm√©todo del codo' ayuda a estimar un buen valor inicial para 'k'. Lo probar√°s en un momento.

## Prerrequisito

Trabajar√°s en el archivo [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/notebook.ipynb) de esta lecci√≥n, que incluye la importaci√≥n de datos y la limpieza preliminar que realizaste en la √∫ltima lecci√≥n.

## Ejercicio - preparaci√≥n

Comienza revisando nuevamente los datos de las canciones.

1. Crea un diagrama de caja, llamando a `boxplot()` para cada columna:

    ```python
    plt.figure(figsize=(20,20), dpi=200)
    
    plt.subplot(4,3,1)
    sns.boxplot(x = 'popularity', data = df)
    
    plt.subplot(4,3,2)
    sns.boxplot(x = 'acousticness', data = df)
    
    plt.subplot(4,3,3)
    sns.boxplot(x = 'energy', data = df)
    
    plt.subplot(4,3,4)
    sns.boxplot(x = 'instrumentalness', data = df)
    
    plt.subplot(4,3,5)
    sns.boxplot(x = 'liveness', data = df)
    
    plt.subplot(4,3,6)
    sns.boxplot(x = 'loudness', data = df)
    
    plt.subplot(4,3,7)
    sns.boxplot(x = 'speechiness', data = df)
    
    plt.subplot(4,3,8)
    sns.boxplot(x = 'tempo', data = df)
    
    plt.subplot(4,3,9)
    sns.boxplot(x = 'time_signature', data = df)
    
    plt.subplot(4,3,10)
    sns.boxplot(x = 'danceability', data = df)
    
    plt.subplot(4,3,11)
    sns.boxplot(x = 'length', data = df)
    
    plt.subplot(4,3,12)
    sns.boxplot(x = 'release_date', data = df)
    ```

    Estos datos son un poco ruidosos: al observar cada columna como un diagrama de caja, puedes ver valores at√≠picos.

    ![valores at√≠picos](../../../../translated_images/boxplots.8228c29dabd0f29227dd38624231a175f411f1d8d4d7c012cb770e00e4fdf8b6.es.png)

Podr√≠as revisar el conjunto de datos y eliminar estos valores at√≠picos, pero eso har√≠a que los datos sean bastante m√≠nimos.

1. Por ahora, elige qu√© columnas usar√°s para tu ejercicio de agrupamiento. Escoge aquellas con rangos similares y codifica la columna `artist_top_genre` como datos num√©ricos:

    ```python
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    
    X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]
    
    y = df['artist_top_genre']
    
    X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])
    
    y = le.transform(y)
    ```

1. Ahora necesitas elegir cu√°ntas agrupaciones apuntar. Sabes que hay 3 g√©neros de canciones que extrajimos del conjunto de datos, as√≠ que probemos con 3:

    ```python
    from sklearn.cluster import KMeans
    
    nclusters = 3 
    seed = 0
    
    km = KMeans(n_clusters=nclusters, random_state=seed)
    km.fit(X)
    
    # Predict the cluster for each data point
    
    y_cluster_kmeans = km.predict(X)
    y_cluster_kmeans
    ```

Ves un arreglo impreso con agrupaciones predichas (0, 1 o 2) para cada fila del dataframe.

1. Usa este arreglo para calcular una 'puntuaci√≥n de silueta':

    ```python
    from sklearn import metrics
    score = metrics.silhouette_score(X, y_cluster_kmeans)
    score
    ```

## Puntuaci√≥n de silueta

Busca una puntuaci√≥n de silueta cercana a 1. Esta puntuaci√≥n var√≠a de -1 a 1, y si la puntuaci√≥n es 1, la agrupaci√≥n es densa y bien separada de otras agrupaciones. Un valor cercano a 0 representa agrupaciones superpuestas con muestras muy cerca del l√≠mite de decisi√≥n de las agrupaciones vecinas. [(Fuente)](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam)

Nuestra puntuaci√≥n es **.53**, justo en el medio. Esto indica que nuestros datos no est√°n particularmente bien adaptados a este tipo de agrupamiento, pero continuemos.

### Ejercicio - construir un modelo

1. Importa `KMeans` y comienza el proceso de agrupamiento.

    ```python
    from sklearn.cluster import KMeans
    wcss = []
    
    for i in range(1, 11):
        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)
    
    ```

    Hay algunas partes aqu√≠ que merecen explicaci√≥n.

    > üéì range: Estas son las iteraciones del proceso de agrupamiento.

    > üéì random_state: "Determina la generaci√≥n de n√∫meros aleatorios para la inicializaci√≥n de centroides." [Fuente](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)

    > üéì WCSS: "suma de cuadrados dentro de la agrupaci√≥n" mide la distancia promedio al cuadrado de todos los puntos dentro de una agrupaci√≥n al centroide de la agrupaci√≥n. [Fuente](https://medium.com/@ODSC/unsupervised-learning-evaluating-clusters-bd47eed175ce).

    > üéì Inercia: Los algoritmos K-Means intentan elegir centroides para minimizar la 'inercia', "una medida de cu√°n coherentes son internamente las agrupaciones." [Fuente](https://scikit-learn.org/stable/modules/clustering.html). El valor se agrega a la variable wcss en cada iteraci√≥n.

    > üéì k-means++: En [Scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#k-means) puedes usar la optimizaci√≥n 'k-means++', que "inicializa los centroides para que est√©n (generalmente) distantes entre s√≠, lo que lleva a resultados probablemente mejores que la inicializaci√≥n aleatoria."

### M√©todo del codo

Anteriormente, dedujiste que, dado que has apuntado a 3 g√©neros de canciones, deber√≠as elegir 3 agrupaciones. ¬øPero es ese el caso?

1. Usa el 'm√©todo del codo' para asegurarte.

    ```python
    plt.figure(figsize=(10,5))
    sns.lineplot(x=range(1, 11), y=wcss, marker='o', color='red')
    plt.title('Elbow')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()
    ```

    Usa la variable `wcss` que construiste en el paso anterior para crear un gr√°fico que muestre d√≥nde est√° el 'doblez' en el codo, lo que indica el n√∫mero √≥ptimo de agrupaciones. ¬°Quiz√°s s√≠ sean 3!

    ![m√©todo del codo](../../../../translated_images/elbow.72676169eed744ff03677e71334a16c6b8f751e9e716e3d7f40dd7cdef674cca.es.png)

## Ejercicio - mostrar las agrupaciones

1. Prueba el proceso nuevamente, esta vez configurando tres agrupaciones, y muestra las agrupaciones como un gr√°fico de dispersi√≥n:

    ```python
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters = 3)
    kmeans.fit(X)
    labels = kmeans.predict(X)
    plt.scatter(df['popularity'],df['danceability'],c = labels)
    plt.xlabel('popularity')
    plt.ylabel('danceability')
    plt.show()
    ```

1. Verifica la precisi√≥n del modelo:

    ```python
    labels = kmeans.labels_
    
    correct_labels = sum(y == labels)
    
    print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))
    
    print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))
    ```

    La precisi√≥n de este modelo no es muy buena, y la forma de las agrupaciones te da una pista de por qu√©.

    ![agrupaciones](../../../../translated_images/clusters.b635354640d8e4fd4a49ef545495518e7be76172c97c13bd748f5b79f171f69a.es.png)

    Estos datos est√°n demasiado desequilibrados, tienen poca correlaci√≥n y hay demasiada variaci√≥n entre los valores de las columnas para agrupar bien. De hecho, las agrupaciones que se forman probablemente est√°n fuertemente influenciadas o sesgadas por las tres categor√≠as de g√©neros que definimos anteriormente. ¬°Eso fue un proceso de aprendizaje!

    En la documentaci√≥n de Scikit-learn, puedes ver que un modelo como este, con agrupaciones no muy bien delimitadas, tiene un problema de 'varianza':

    ![modelos problem√°ticos](../../../../translated_images/problems.f7fb539ccd80608e1f35c319cf5e3ad1809faa3c08537aead8018c6b5ba2e33a.es.png)
    > Infograf√≠a de Scikit-learn

## Varianza

La varianza se define como "el promedio de las diferencias al cuadrado respecto a la media" [(Fuente)](https://www.mathsisfun.com/data/standard-deviation.html). En el contexto de este problema de agrupamiento, se refiere a datos cuyos n√∫meros tienden a divergir demasiado de la media.

‚úÖ Este es un gran momento para pensar en todas las formas en que podr√≠as corregir este problema. ¬øModificar un poco m√°s los datos? ¬øUsar diferentes columnas? ¬øProbar un algoritmo diferente? Pista: Intenta [escalar tus datos](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/) para normalizarlos y prueba otras columnas.

> Prueba este '[calculador de varianza](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)' para entender mejor el concepto.

---

## üöÄDesaf√≠o

Dedica tiempo a este notebook ajustando par√°metros. ¬øPuedes mejorar la precisi√≥n del modelo limpiando m√°s los datos (eliminando valores at√≠picos, por ejemplo)? Puedes usar pesos para dar m√°s importancia a ciertas muestras de datos. ¬øQu√© m√°s puedes hacer para crear mejores agrupaciones?

Pista: Intenta escalar tus datos. Hay c√≥digo comentado en el notebook que agrega escalado est√°ndar para hacer que las columnas de datos se parezcan m√°s entre s√≠ en t√©rminos de rango. Descubrir√°s que, aunque la puntuaci√≥n de silueta disminuye, el 'doblez' en el gr√°fico del codo se suaviza. Esto se debe a que dejar los datos sin escalar permite que los datos con menos varianza tengan m√°s peso. Lee un poco m√°s sobre este problema [aqu√≠](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).

## [Cuestionario posterior a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/30/)

## Revisi√≥n y autoestudio

Echa un vistazo a un simulador de K-Means [como este](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). Puedes usar esta herramienta para visualizar puntos de datos de muestra y determinar sus centroides. Puedes editar la aleatoriedad de los datos, el n√∫mero de agrupaciones y el n√∫mero de centroides. ¬øEsto te ayuda a tener una idea de c√≥mo se pueden agrupar los datos?

Tambi√©n, revisa [este documento sobre K-Means](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) de Stanford.

## Tarea

[Prueba diferentes m√©todos de agrupamiento](assignment.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.