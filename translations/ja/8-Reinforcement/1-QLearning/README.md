<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0ffe994d1cc881bdeb49226a064116e5",
  "translation_date": "2025-09-04T00:17:06+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "ja"
}
-->
# 強化学習とQ-Learningの紹介

![機械学習における強化学習の概要を示すスケッチノート](../../../../translated_images/ml-reinforcement.94024374d63348dbb3571c343ca7ddabef72adac0b8086d47164b769ba3a8a1d.ja.png)
> スケッチノート: [Tomomi Imura](https://www.twitter.com/girlie_mac)

強化学習には、エージェント、いくつかの状態、そして各状態における一連のアクションという3つの重要な概念があります。特定の状態でアクションを実行することで、エージェントは報酬を得ます。例えば、コンピュータゲーム「スーパーマリオ」を想像してみてください。あなたはマリオで、ゲームのレベル内で崖の端に立っています。上にはコインがあります。マリオとして、ゲームレベル内の特定の位置にいる状態が「状態」です。右に1歩進む（アクション）と崖から落ちてしまい、低い数値のスコアを得ることになります。しかし、ジャンプボタンを押すとポイントを獲得し、生き残ることができます。これはポジティブな結果であり、ポジティブな数値スコアが与えられるべきです。

強化学習とシミュレーター（ゲーム）を使用することで、ゲームをプレイして報酬を最大化する方法、つまり生き残りながらできるだけ多くのポイントを獲得する方法を学ぶことができます。

[![強化学習の紹介](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> 🎥 上の画像をクリックして、Dmitryが強化学習について話す様子を視聴してください

## [講義前のクイズ](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/45/)

## 前提条件とセットアップ

このレッスンでは、Pythonでいくつかのコードを試してみます。このレッスンのJupyter Notebookコードを、コンピュータ上またはクラウド上で実行できる必要があります。

[レッスンノートブック](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb)を開き、このレッスンを進めながら構築してください。

> **Note:** クラウドからこのコードを開く場合、ノートブックコードで使用される[`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py)ファイルも取得する必要があります。このファイルをノートブックと同じディレクトリに追加してください。

## はじめに

このレッスンでは、ロシアの作曲家[セルゲイ・プロコフィエフ](https://en.wikipedia.org/wiki/Sergei_Prokofiev)による音楽童話「[ピーターと狼](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)」にインスパイアされた世界を探求します。**強化学習**を使用して、ピーターが環境を探索し、美味しいリンゴを集め、狼に遭遇しないようにする方法を学びます。

**強化学習**（RL）は、**エージェント**がある**環境**内で最適な行動を学ぶための学習手法で、多くの実験を通じて学習します。この環境内のエージェントは、**報酬関数**によって定義された**目標**を持つ必要があります。

## 環境

簡単のため、ピーターの世界を`幅` x `高さ`の正方形のボードと考えます。このような感じです：

![ピーターの環境](../../../../translated_images/environment.40ba3cb66256c93fa7e92f6f7214e1d1f588aafa97d266c11d108c5c5d101b6c.ja.png)

このボードの各セルは以下のいずれかになります：

* **地面**: ピーターや他の生物が歩ける場所。
* **水**: 明らかに歩けない場所。
* **木**または**草**: 休憩できる場所。
* **リンゴ**: ピーターが見つけて喜ぶ食べ物。
* **狼**: 危険で避けるべき存在。

この環境で作業するためのコードを含む、別のPythonモジュール[`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py)があります。このコードは概念を理解する上で重要ではないため、モジュールをインポートしてサンプルボードを作成します（コードブロック1）：

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

このコードは、上記のような環境の画像を出力するはずです。

## アクションとポリシー

この例では、ピーターの目標は狼や他の障害物を避けながらリンゴを見つけることです。そのため、彼は基本的に歩き回ってリンゴを見つける必要があります。

したがって、任意の位置で、彼は以下のアクションのいずれかを選択できます：上、下、左、右。

これらのアクションを辞書として定義し、それを対応する座標変化のペアにマッピングします。例えば、右に移動する（`R`）はペア`(1,0)`に対応します。（コードブロック2）：

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

このシナリオの戦略と目標をまとめると以下の通りです：

- **戦略**: エージェント（ピーター）の戦略は、いわゆる**ポリシー**によって定義されます。ポリシーは、任意の状態でアクションを返す関数です。この場合、問題の状態はボードとプレイヤーの現在位置によって表されます。

- **目標**: 強化学習の目標は、問題を効率的に解決するための良いポリシーを最終的に学ぶことです。ただし、ベースラインとして、最も単純なポリシーである**ランダムウォーク**を考えます。

## ランダムウォーク

まず、ランダムウォーク戦略を実装して問題を解決してみましょう。ランダムウォークでは、許可されたアクションから次のアクションをランダムに選択し、リンゴに到達するまで繰り返します（コードブロック3）。

1. 以下のコードを使用してランダムウォークを実装します：

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    `walk`の呼び出しは、対応する経路の長さを返すはずです。この長さは実行ごとに異なる場合があります。

1. ウォーク実験を複数回（例えば100回）実行し、結果の統計を出力します（コードブロック4）：

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    経路の平均長さは約30〜40ステップであることに注意してください。これは、最寄りのリンゴまでの平均距離が約5〜6ステップであることを考えるとかなり多いです。

    また、ランダムウォーク中のピーターの動きを以下のように視覚化することもできます：

    ![ピーターのランダムウォーク](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## 報酬関数

ポリシーをより賢くするためには、どの動きが他より「良い」のかを理解する必要があります。そのためには、目標を定義する必要があります。

目標は、各状態に対してスコア値を返す**報酬関数**によって定義できます。この数値が高いほど、報酬関数は良いとされます。（コードブロック5）

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

報酬関数の興味深い点は、多くの場合、*ゲームの最後にのみ実質的な報酬が与えられる*ということです。つまり、アルゴリズムは最終的なポジティブな報酬につながる「良い」ステップを覚えてその重要性を高める必要があります。同様に、悪い結果につながるすべての動きは抑制されるべきです。

## Q-Learning

ここで説明するアルゴリズムは**Q-Learning**と呼ばれます。このアルゴリズムでは、ポリシーは**Q-テーブル**と呼ばれる関数（またはデータ構造）によって定義されます。これは、特定の状態での各アクションの「良さ」を記録します。

Q-テーブルと呼ばれるのは、テーブルや多次元配列として表現するのが便利だからです。ボードの寸法が`幅` x `高さ`であるため、Q-テーブルは形状`幅` x `高さ` x `len(actions)`のnumpy配列として表現できます。（コードブロック6）

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Q-テーブルのすべての値を等しい値（この場合は0.25）で初期化することに注意してください。これは「ランダムウォーク」ポリシーに対応します。なぜなら、各状態でのすべての動きが等しく良いとされるからです。Q-テーブルを`plot`関数に渡して、ボード上でテーブルを視覚化できます：`m.plot(Q)`。

![ピーターの環境](../../../../translated_images/env_init.04e8f26d2d60089e128f21d22e5fef57d580e559f0d5937b06c689e5e7cdd438.ja.png)

各セルの中央には、移動の好ましい方向を示す「矢印」があります。すべての方向が等しい場合、点が表示されます。

次に、シミュレーションを実行し、環境を探索し、Q-テーブルの値のより良い分布を学習します。これにより、リンゴへの経路をはるかに速く見つけることができるようになります。

## Q-Learningの本質: ベルマン方程式

移動を開始すると、各アクションには対応する報酬があります。つまり、理論的には、最も高い即時報酬に基づいて次のアクションを選択できます。しかし、ほとんどの状態では、移動がリンゴに到達するという目標を達成するわけではないため、どの方向が良いかを即座に判断することはできません。

> 即時の結果ではなく、シミュレーションの最後に得られる最終的な結果が重要です。

この遅延報酬を考慮するために、**[動的計画法](https://en.wikipedia.org/wiki/Dynamic_programming)**の原則を使用します。これにより、問題を再帰的に考えることができます。

現在の状態を*s*とし、次の状態*s'*に移動したいとします。このとき、報酬関数によって定義される即時報酬*r(s,a)*を受け取り、さらに将来の報酬を得ることになります。Q-テーブルが各アクションの「魅力」を正確に反映していると仮定すると、状態*s'*では*Q(s',a')*の最大値に対応するアクション*a'*を選択します。したがって、状態*s*で得られる可能性のある最良の将来の報酬は、`max`

## ポリシーの確認

Q-Tableは各状態での各アクションの「魅力」を示しているため、これを使って効率的なナビゲーションを定義するのは非常に簡単です。最も単純な場合、Q-Tableの値が最も高いアクションを選択します。（コードブロック 9）

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> 上記のコードを何度か試してみると、時々「停止」してしまい、ノートブックのSTOPボタンを押して中断する必要があることに気付くかもしれません。これは、2つの状態が最適なQ-Valueの観点で互いに「指し示す」状況があり、その場合、エージェントがその状態間を無限に移動し続けるためです。

## 🚀チャレンジ

> **タスク 1:** `walk`関数を修正して、パスの最大長を特定のステップ数（例えば100）で制限し、上記のコードが時々この値を返す様子を観察してください。

> **タスク 2:** `walk`関数を修正して、以前に訪れた場所に戻らないようにしてください。これにより`walk`がループするのを防ぐことができますが、エージェントが脱出できない場所に「閉じ込められる」可能性は依然としてあります。

## ナビゲーション

より良いナビゲーションポリシーは、トレーニング中に使用したものです。これは、利用と探索を組み合わせたものです。このポリシーでは、Q-Tableの値に比例した確率で各アクションを選択します。この戦略では、エージェントがすでに探索した位置に戻る可能性はありますが、以下のコードからわかるように、目的地までの平均パスが非常に短くなります（`print_statistics`はシミュレーションを100回実行します）：（コードブロック 10）

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

このコードを実行すると、以前よりもはるかに短い平均パス長が得られ、3〜6の範囲になります。

## 学習プロセスの調査

前述の通り、学習プロセスは探索と問題空間の構造に関する知識の活用のバランスです。学習の結果（エージェントが目標への短いパスを見つける能力）が改善されたことがわかりますが、学習プロセス中の平均パス長の挙動を観察するのも興味深いです：

学習内容を以下のようにまとめることができます：

- **平均パス長の増加**。最初は平均パス長が増加することが見られます。これは、環境について何も知らない場合、悪い状態（水や狼）に閉じ込められる可能性が高いためです。より多くを学び、この知識を使い始めると、環境を長く探索できるようになりますが、まだリンゴの場所をよく知らない状態です。

- **学習が進むにつれてパス長が減少**。十分に学習すると、エージェントが目標を達成するのが簡単になり、パス長が減少し始めます。ただし、まだ探索を続けているため、最適なパスから外れて新しい選択肢を探索することがあり、パスが最適よりも長くなることがあります。

- **突然のパス長の増加**。グラフで観察されるもう一つの特徴は、ある時点でパス長が突然増加することです。これはプロセスの確率的な性質を示しており、Q-Tableの係数が新しい値で上書きされることで「損なわれる」可能性があることを意味します。これを最小化するには、学習率を減少させる（例えば、トレーニングの終盤ではQ-Tableの値を小さな値でのみ調整する）ことが理想的です。

全体として、学習プロセスの成功と質は、学習率、学習率の減衰、割引率などのパラメータに大きく依存することを覚えておくことが重要です。これらは**ハイパーパラメータ**と呼ばれ、トレーニング中に最適化する**パラメータ**（例えば、Q-Tableの係数）とは区別されます。最適なハイパーパラメータ値を見つけるプロセスは**ハイパーパラメータ最適化**と呼ばれ、別のトピックとして扱う価値があります。

## [講義後のクイズ](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/46/)

## 課題 
[より現実的な世界](assignment.md)

---

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を期すよう努めておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があります。元の言語で記載された原文が正式な情報源と見なされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤認について、当社は一切の責任を負いません。