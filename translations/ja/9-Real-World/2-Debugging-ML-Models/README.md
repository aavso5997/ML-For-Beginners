<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ba0f6e1019351351c8ee4c92867b6a0b",
  "translation_date": "2025-09-03T23:20:45+00:00",
  "source_file": "9-Real-World/2-Debugging-ML-Models/README.md",
  "language_code": "ja"
}
-->
# 後書き: 責任あるAIダッシュボードコンポーネントを使用した機械学習モデルのデバッグ

## [講義前のクイズ](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)

## はじめに

機械学習は私たちの日常生活に影響を与えています。AIは、医療、金融、教育、雇用など、個人や社会に影響を与える重要なシステムに浸透しています。例えば、医療診断や詐欺検出など、日々の意思決定に関わるシステムやモデルが存在します。このようにAIの進歩と急速な普及は、社会的期待の変化や規制の強化を伴っています。AIシステムが期待を裏切る場面が頻繁に見られ、新たな課題が浮き彫りになり、政府はAIソリューションの規制を始めています。そのため、これらのモデルを分析し、公平で信頼性があり、包括的で透明性が高く、責任ある結果を提供することが重要です。

このカリキュラムでは、モデルに責任あるAIの問題があるかどうかを評価するための実用的なツールを紹介します。従来の機械学習デバッグ技術は、集計された精度や平均誤差損失などの定量的計算に基づいていることが多いです。しかし、モデル構築に使用するデータが人種、性別、政治的見解、宗教などの特定の属性を欠いている場合や、これらの属性が偏って表現されている場合に何が起こるかを想像してみてください。また、モデルの出力が特定の属性を優遇するように解釈される場合はどうでしょうか。このような状況は、モデルの公平性、包括性、信頼性に問題を引き起こす可能性があります。さらに、機械学習モデルはブラックボックスと見なされることが多く、モデルの予測を駆動する要因を理解し説明することが難しいです。これらは、モデルの公平性や信頼性を評価するための適切なツールを持たないデータサイエンティストやAI開発者が直面する課題です。

このレッスンでは、以下の方法でモデルをデバッグする方法を学びます：

- **エラー分析**: データ分布内でモデルのエラー率が高い箇所を特定する。
- **モデル概要**: 異なるデータコホート間で比較分析を行い、モデルのパフォーマンス指標の格差を発見する。
- **データ分析**: データの過剰または不足表現がモデルを特定のデータ属性に偏らせる可能性がある箇所を調査する。
- **特徴の重要性**: モデルの予測をグローバルまたはローカルレベルで駆動する特徴を理解する。

## 前提条件

前提条件として、[開発者向け責任あるAIツール](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)をレビューしてください。

> ![責任あるAIツールのGIF](../../../../9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## エラー分析

従来のモデルパフォーマンス指標は、正確性を測定するために正しい予測と誤った予測に基づく計算が主流です。例えば、モデルが89%の精度でエラー損失が0.001であると判断される場合、良好なパフォーマンスと見なされることがあります。しかし、エラーは基礎となるデータセット内で均一に分布しているわけではありません。モデルの精度が89%であっても、データの特定の領域でモデルが42%の確率で失敗していることが判明する場合があります。このような特定のデータグループにおける失敗パターンの結果は、公平性や信頼性の問題につながる可能性があります。モデルが良好に機能している箇所やそうでない箇所を理解することが重要です。モデルの不正確さが多いデータ領域は、重要なデータ属性である可能性があります。

![モデルエラーを分析・デバッグ](../../../../translated_images/ea-error-distribution.117452e1177c1dd84fab2369967a68bcde787c76c6ea7fdb92fcf15d1fce8206.ja.png)

RAIダッシュボードのエラー分析コンポーネントは、ツリーの視覚化を通じてモデルの失敗がさまざまなコホートにどのように分布しているかを示します。これにより、データセット内でエラー率が高い特徴や領域を特定するのに役立ちます。モデルの不正確さの大部分がどこから来ているかを確認することで、根本原因の調査を開始できます。また、データコホートを作成して分析を行うことも可能です。これらのデータコホートは、あるコホートでモデルのパフォーマンスが良好である理由や、別のコホートで誤りが発生する理由を特定するデバッグプロセスに役立ちます。

![エラー分析](../../../../translated_images/ea-error-cohort.6886209ea5d438c4daa8bfbf5ce3a7042586364dd3eccda4a4e3d05623ac702a.ja.png)

ツリーマップの視覚的指標は、問題箇所を迅速に特定するのに役立ちます。例えば、ツリーノードの赤色が濃いほど、エラー率が高いことを示します。

ヒートマップは、1つまたは2つの特徴を使用してエラー率を調査し、データセット全体やコホート内でモデルエラーの原因を特定するための別の視覚化機能です。

![エラー分析ヒートマップ](../../../../translated_images/ea-heatmap.8d27185e28cee3830c85e1b2e9df9d2d5e5c8c940f41678efdb68753f2f7e56c.ja.png)

エラー分析を使用する場面：

* モデルの失敗がデータセット全体や複数の入力・特徴次元にどのように分布しているかを深く理解する。
* 集計パフォーマンス指標を分解し、誤りのあるコホートを自動的に発見して、ターゲットを絞った緩和策を講じる。

## モデル概要

機械学習モデルのパフォーマンスを評価するには、その挙動を包括的に理解する必要があります。これを達成するには、エラー率、精度、リコール、精度、MAE（平均絶対誤差）など、複数の指標を確認し、パフォーマンス指標間の格差を見つけることが重要です。1つの指標が良好に見えても、別の指標で不正確さが露呈することがあります。さらに、データセット全体やコホート間で指標を比較して格差を確認することで、モデルが良好に機能している箇所やそうでない箇所を明らかにすることができます。特に、敏感な特徴（例：患者の人種、性別、年齢）と非敏感な特徴の間でモデルのパフォーマンスを確認することで、モデルが持つ潜在的な不公平性を明らかにすることができます。例えば、敏感な特徴を持つコホートでモデルがより誤りを犯していることを発見することで、モデルの不公平性が明らかになる場合があります。

RAIダッシュボードのモデル概要コンポーネントは、データコホート内のデータ表現のパフォーマンス指標を分析するだけでなく、異なるコホート間でモデルの挙動を比較する機能を提供します。

![RAIダッシュボードのデータセットコホート - モデル概要](../../../../translated_images/model-overview-dataset-cohorts.dfa463fb527a35a0afc01b7b012fc87bf2cad756763f3652bbd810cac5d6cf33.ja.png)

このコンポーネントの特徴ベースの分析機能により、特定の特徴内のデータサブグループを絞り込み、詳細レベルで異常を特定することができます。例えば、ダッシュボードには、ユーザーが選択した特徴（例：*"time_in_hospital < 3"* または *"time_in_hospital >= 7"*）に基づいてコホートを自動生成するインテリジェンスが組み込まれています。これにより、ユーザーは大規模なデータグループから特定の特徴を分離し、それがモデルの誤った結果の主要な影響要因であるかどうかを確認できます。

![RAIダッシュボードの特徴コホート - モデル概要](../../../../translated_images/model-overview-feature-cohorts.c5104d575ffd0c80b7ad8ede7703fab6166bfc6f9125dd395dcc4ace2f522f70.ja.png)

モデル概要コンポーネントは、以下の2種類の格差指標をサポートしています：

**モデルパフォーマンスの格差**: これらの指標は、選択したパフォーマンス指標の値の格差（差異）をデータのサブグループ間で計算します。以下はその例です：

* 精度率の格差
* エラー率の格差
* 精度の格差
* リコールの格差
* 平均絶対誤差（MAE）の格差

**選択率の格差**: この指標は、サブグループ間での選択率（好ましい予測）の差異を含みます。例えば、ローン承認率の格差がこれに該当します。選択率とは、各クラスのデータポイントのうち、1（バイナリ分類の場合）に分類された割合や、予測値の分布（回帰の場合）を指します。

## データ分析

> 「データを十分に拷問すれば、何でも白状する」 - ロナルド・コース

この言葉は極端に聞こえるかもしれませんが、データはどんな結論を支持するようにも操作可能であるという点で真実です。このような操作は時に意図せずに行われることもあります。人間は誰しもバイアスを持っており、データにバイアスを導入していることを意識的に認識するのは難しいことがあります。AIや機械学習における公平性を保証することは依然として複雑な課題です。

データは従来のモデルパフォーマンス指標にとって大きな盲点です。高い精度スコアを持っていても、それがデータセット内の潜在的なデータバイアスを反映しているとは限りません。例えば、ある企業の役員ポジションにおける従業員のデータセットが、女性が27%、男性が73%である場合、このデータでトレーニングされた求人広告AIモデルは、主に男性を対象にしたシニアレベルの求人をターゲットにする可能性があります。このようなデータの不均衡は、モデルの予測を特定の性別に偏らせ、公平性の問題を引き起こします。

RAIダッシュボードのデータ分析コンポーネントは、データセット内で過剰または不足している表現を特定するのに役立ちます。これにより、データの不均衡や特定のデータグループの表現不足から生じるエラーや公平性の問題の根本原因を診断することができます。ユーザーは予測結果や実際の結果、エラーグループ、特定の特徴に基づいてデータセットを視覚化することができます。時には、表現不足のデータグループを発見することで、モデルが十分に学習していないことが明らかになり、それが高い不正確さの原因である場合もあります。データバイアスを持つモデルは、公平性の問題だけでなく、包括性や信頼性の欠如を示しています。

![RAIダッシュボードのデータ分析コンポーネント](../../../../translated_images/dataanalysis-cover.8d6d0683a70a5c1e274e5a94b27a71137e3d0a3b707761d7170eb340dd07f11d.ja.png)

データ分析を使用する場面：

* 異なるフィルターを選択してデータをさまざまな次元（コホート）に分割し、データセット統計を探索する。
* データセットの分布を異なるコホートや特徴グループ間で理解する。
* 公平性、エラー分析、因果関係（他のダッシュボードコンポーネントから得られる）に関連する発見がデータセットの分布によるものかどうかを判断する。
* 表現問題、ラベルノイズ、特徴ノイズ、ラベルバイアスなどの要因から生じるエラーを緩和するために、どの領域でデータを収集するべきかを決定する。

## モデルの解釈性

機械学習モデルはブラックボックスと見なされることが多いです。モデルの予測を駆動する主要なデータ特徴を理解することは困難です。モデルが特定の予測を行う理由に透明性を提供することが重要です。例えば、AIシステムが糖尿病患者が30日以内に再入院するリスクがあると予測した場合、その予測に至った根拠となるデータを提供するべきです。このような根拠データの指標は、医師や病院が十分な情報に基づいた意思決定を行うのに役立ちます。また、個々の患者に対するモデルの予測理由を説明することで、医療規制に対する責任を果たすことができます。人々の生活に影響を与える方法で機械学習モデルを使用する場合、モデルの挙動を駆動する要因を理解し説明することが重要です。モデルの説明可能性と解釈性は、以下のようなシナリオで疑問に答えるのに役立ちます：

* モデルのデバッグ: なぜモデルがこの誤りを犯したのか？モデルをどのように改善できるのか？
* 人間とAIの協力: モデルの決定をどのように理解し信頼できるのか？
* 規制遵守: モデルは法的要件を満たしているか？

RAIダッシュボードの特徴の重要性コンポーネントは、モデルの予測がどのように行われるかを包括的に理解しデバッグするのに役立ちます。また、機械学習の専門家や意思決定者がモデルの挙動に影響を与える特徴の証拠を説明し、規制遵守のために示すのに役立つツールです。次に、ユーザーはグローバルおよびローカルの説明を探索し、モデルの予測を駆動する特徴を検証できます。グローバル説明は、モデルの全体的な予測に影響を与えた主要な特徴をリストします。ローカル説明は、個々のケースに対するモデルの予測理由を表示します。ローカル説明を評価する能力は、特定のケースをデバッグまたは監査し、モデルが正確または不正確な予測を行った理由をよりよく理解し解釈するのに役立ちます。

![RAIダッシュボードの特徴の重要性コンポーネント](../../../../translated_images/9-feature-importance.cd3193b4bba3fd4bccd415f566c2437fb3298c4824a3dabbcab15270d783606e.ja.png)

* グローバル説明: 例えば、糖尿病患者の病院再入院モデルの全体的な挙動に影響を与える特徴は何か？
* ローカル説明: 例えば、60歳以上で以前に入院した糖尿病患者が30日以内に再入院すると予測された理由は何か？

異なるコホート間でモデルのパフォーマンスを調査するデバッグプロセスでは、特徴の重要性がコホート全体で特徴がどの程度影響を与えているかを示します。特徴がモデルの誤った予測を駆動する影響度を比較する際に異常を明らかにするのに役立ちます。このコンポーネントは、特徴や特徴値がモデルの結果にどのように正または負の影響を与えたかを示すことができます。例えば、モデルが不正確な予測を行った場合、このコンポーネントは予測を駆動した特徴や特徴値を特定する能力を提供します。この詳細レベルは、デバッグだけでなく、監査状況での透明性と責任を提供するのに役立ちます。最後に、このコンポーネントは公平性の問題を特定するのにも役立ちます。例えば、民族性や性別などの敏感な特徴がモデルの予測を駆動する主要な影響要因である場合、これはモデルに人種や性別のバイアスがある兆候である可能性があります。

![特徴の重要性](../../../../translated_images/9-features-influence.3ead3d3f68a84029f1e40d3eba82107445d3d3b6975d4682b23d8acc905da6d0.ja.png)

解釈性を使用する場面：

* AIシステムの予測がどの程度信頼できるかを判断するために、予測に最も重要な特徴を理解する。
* モデルを
- **過剰または不足の表現**。特定のグループがある職業で見られない場合、その状況を促進し続けるサービスや機能は害を与えることに寄与していると考えられます。

### Azure RAI ダッシュボード

[Azure RAI ダッシュボード](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) は、Microsoftを含む主要な学術機関や組織によって開発されたオープンソースツールを基盤としており、データサイエンティストやAI開発者がモデルの挙動をより深く理解し、AIモデルから生じる望ましくない問題を発見し軽減するのに役立ちます。

- RAIダッシュボードの異なるコンポーネントの使い方を学ぶには、[ドキュメント](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)をチェックしてください。

- Azure Machine Learningでより責任あるAIシナリオをデバッグするためのRAIダッシュボードの[サンプルノートブック](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks)を確認してください。

---
## 🚀 チャレンジ

統計的またはデータバイアスが最初から導入されるのを防ぐために、以下を行うべきです：

- システムに取り組む人々の背景や視点の多様性を確保する
- 社会の多様性を反映したデータセットに投資する
- バイアスが発生した際にそれを検出し修正するためのより良い方法を開発する

モデル構築や使用において不公平が明らかになる実際のシナリオについて考えてみてください。他に何を考慮すべきでしょうか？

## [講義後のクイズ](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)
## 復習と自己学習

このレッスンでは、機械学習に責任あるAIを組み込むための実践的なツールについて学びました。

以下のワークショップを視聴して、トピックをさらに深く掘り下げてください：

- Besmira NushiとMehrnoosh Samekiによる「Responsible AI Dashboard: 実践でRAIを運用するためのワンストップショップ」

[![Responsible AI Dashboard: 実践でRAIを運用するためのワンストップショップ](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "Responsible AI Dashboard: 実践でRAIを運用するためのワンストップショップ")

> 🎥 上の画像をクリックして動画を視聴してください: Besmira NushiとMehrnoosh Samekiによる「Responsible AI Dashboard: 実践でRAIを運用するためのワンストップショップ」

責任あるAIについてさらに学び、より信頼性の高いモデルを構築する方法を知るために以下の資料を参照してください：

- MLモデルをデバッグするためのMicrosoftのRAIダッシュボードツール: [責任あるAIツールのリソース](https://aka.ms/rai-dashboard)

- 責任あるAIツールキットを探索する: [Github](https://github.com/microsoft/responsible-ai-toolbox)

- MicrosoftのRAIリソースセンター: [責任あるAIリソース – Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- MicrosoftのFATE研究グループ: [FATE: AIにおける公平性、説明責任、透明性、倫理 - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

## 課題

[RAIダッシュボードを探索する](assignment.md)

---

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知ください。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解釈について、当方は一切の責任を負いません。