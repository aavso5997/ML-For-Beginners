<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ba0f6e1019351351c8ee4c92867b6a0b",
  "translation_date": "2025-09-03T23:21:34+00:00",
  "source_file": "9-Real-World/2-Debugging-ML-Models/README.md",
  "language_code": "ko"
}
-->
# 후기: Responsible AI 대시보드 구성 요소를 활용한 머신러닝 모델 디버깅

## [강의 전 퀴즈](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)

## 소개

머신러닝은 우리의 일상에 큰 영향을 미치고 있습니다. AI는 의료, 금융, 교육, 고용 등 개인과 사회에 중요한 영향을 미치는 시스템에 점점 더 많이 도입되고 있습니다. 예를 들어, 의료 진단이나 사기 탐지와 같은 일상적인 의사결정 작업에 시스템과 모델이 사용됩니다. 이처럼 AI의 발전과 빠른 채택은 사회적 기대의 변화와 규제의 증가를 불러오고 있습니다. 우리는 AI 시스템이 기대에 미치지 못하거나 새로운 문제를 드러내는 사례를 지속적으로 목격하고 있으며, 정부는 AI 솔루션을 규제하기 시작했습니다. 따라서 이러한 모델이 공정하고, 신뢰할 수 있으며, 포괄적이고, 투명하며, 책임 있는 결과를 제공하는지 분석하는 것이 중요합니다.

이 커리큘럼에서는 모델이 Responsible AI 문제를 가지고 있는지 평가하는 데 사용할 수 있는 실용적인 도구를 살펴봅니다. 전통적인 머신러닝 디버깅 기법은 주로 집계된 정확도나 평균 오류 손실과 같은 정량적 계산에 기반합니다. 예를 들어, 모델을 구축하는 데 사용하는 데이터가 특정 인구통계(예: 인종, 성별, 정치적 견해, 종교 등)가 부족하거나 불균형적으로 대표된다면 어떤 일이 발생할 수 있을까요? 또는 모델의 출력이 특정 인구통계를 선호하도록 해석된다면 어떨까요? 이는 민감한 특성 그룹의 과대 또는 과소 대표를 초래하여 모델의 공정성, 포괄성 또는 신뢰성 문제를 야기할 수 있습니다. 또 다른 문제는 머신러닝 모델이 블랙박스로 간주되어 모델의 예측을 이끄는 요인을 이해하고 설명하기 어렵다는 점입니다. 이러한 문제는 데이터 과학자와 AI 개발자가 모델의 공정성이나 신뢰성을 평가하고 디버깅할 적절한 도구가 없을 때 직면하는 도전 과제입니다.

이 강의에서는 다음을 통해 모델을 디버깅하는 방법을 배웁니다:

- **오류 분석**: 데이터 분포에서 모델의 오류율이 높은 부분을 식별합니다.
- **모델 개요**: 다양한 데이터 코호트 간의 비교 분석을 수행하여 모델 성능 지표의 격차를 발견합니다.
- **데이터 분석**: 데이터가 특정 인구통계를 과대 또는 과소 대표하여 모델이 특정 데이터를 선호하도록 왜곡될 수 있는 부분을 조사합니다.
- **특성 중요도**: 모델의 예측에 영향을 미치는 주요 특성을 전역 수준 또는 로컬 수준에서 이해합니다.

## 사전 요구 사항

사전 요구 사항으로, [개발자를 위한 Responsible AI 도구](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)를 검토하세요.

> ![Responsible AI 도구에 대한 GIF](../../../../9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## 오류 분석

전통적인 모델 성능 지표는 주로 올바른 예측과 잘못된 예측을 기반으로 한 계산입니다. 예를 들어, 모델이 89%의 정확도를 가지며 오류 손실이 0.001인 경우 좋은 성능으로 간주될 수 있습니다. 그러나 오류는 데이터셋 전반에 균일하게 분포하지 않을 수 있습니다. 모델이 89%의 정확도를 기록했더라도 특정 데이터 영역에서 모델이 42%의 오류율을 보이는 경우를 발견할 수 있습니다. 특정 데이터 그룹에서 이러한 오류 패턴이 발생하면 공정성이나 신뢰성 문제로 이어질 수 있습니다. 따라서 모델이 잘 작동하는 영역과 그렇지 않은 영역을 이해하는 것이 중요합니다. 모델의 부정확성이 높은 데이터 영역은 중요한 데이터 인구통계일 수 있습니다.

![모델 오류 분석 및 디버깅](../../../../translated_images/ea-error-distribution.117452e1177c1dd84fab2369967a68bcde787c76c6ea7fdb92fcf15d1fce8206.ko.png)

RAI 대시보드의 오류 분석 구성 요소는 트리 시각화를 통해 모델 오류가 다양한 코호트에 어떻게 분포되어 있는지 보여줍니다. 이를 통해 데이터셋에서 오류율이 높은 특성이나 영역을 식별하는 데 유용합니다. 모델의 부정확성이 주로 어디에서 발생하는지 확인함으로써 근본 원인을 조사할 수 있습니다. 또한 데이터를 코호트로 나누어 분석을 수행할 수 있습니다. 이러한 데이터 코호트는 특정 코호트에서 모델 성능이 좋은 이유와 다른 코호트에서 오류가 발생하는 이유를 파악하는 데 도움을 줍니다.

![오류 분석](../../../../translated_images/ea-error-cohort.6886209ea5d438c4daa8bfbf5ce3a7042586364dd3eccda4a4e3d05623ac702a.ko.png)

트리 맵의 시각적 표시기는 문제 영역을 더 빠르게 찾을 수 있도록 도와줍니다. 예를 들어, 트리 노드의 빨간색 음영이 짙을수록 오류율이 높습니다.

히트맵은 데이터셋 전체 또는 코호트에서 모델 오류의 기여 요인을 찾기 위해 하나 또는 두 개의 특성을 사용하여 오류율을 조사하는 또 다른 시각화 기능입니다.

![오류 분석 히트맵](../../../../translated_images/ea-heatmap.8d27185e28cee3830c85e1b2e9df9d2d5e5c8c940f41678efdb68753f2f7e56c.ko.png)

오류 분석을 사용해야 할 때:

- 데이터셋 및 여러 입력 및 특성 차원에서 모델 오류가 어떻게 분포되어 있는지 깊이 이해하고자 할 때.
- 집계된 성능 지표를 세분화하여 오류가 있는 코호트를 자동으로 발견하고 이를 기반으로 목표 지향적인 완화 단계를 계획하고자 할 때.

## 모델 개요

머신러닝 모델의 성능을 평가하려면 모델의 행동을 전체적으로 이해해야 합니다. 이는 오류율, 정확도, 재현율, 정밀도, MAE(평균 절대 오차)와 같은 여러 지표를 검토하여 성능 지표 간의 격차를 발견함으로써 가능합니다. 하나의 성능 지표는 훌륭해 보일 수 있지만, 다른 지표에서 부정확성이 드러날 수 있습니다. 또한, 전체 데이터셋이나 코호트 간의 지표를 비교하면 모델이 잘 작동하는 영역과 그렇지 않은 영역을 파악하는 데 도움이 됩니다. 특히, 민감한 특성과 비민감한 특성(예: 환자의 인종, 성별, 나이) 간의 모델 성능을 비교하면 모델이 잠재적으로 불공정한 부분을 발견할 수 있습니다. 예를 들어, 민감한 특성을 가진 코호트에서 모델이 더 많은 오류를 보인다면, 이는 모델이 불공정할 가능성을 나타낼 수 있습니다.

RAI 대시보드의 모델 개요 구성 요소는 데이터 코호트에서의 데이터 표현 성능 지표를 분석할 뿐만 아니라, 사용자가 서로 다른 코호트 간의 모델 행동을 비교할 수 있는 기능을 제공합니다.

![RAI 대시보드의 데이터셋 코호트 - 모델 개요](../../../../translated_images/model-overview-dataset-cohorts.dfa463fb527a35a0afc01b7b012fc87bf2cad756763f3652bbd810cac5d6cf33.ko.png)

이 구성 요소의 특성 기반 분석 기능은 사용자가 특정 특성 내의 데이터 하위 그룹을 세분화하여 세부적으로 이상치를 식별할 수 있도록 합니다. 예를 들어, 대시보드는 사용자가 선택한 특성(예: *"time_in_hospital < 3"* 또는 *"time_in_hospital >= 7"*)에 대해 자동으로 코호트를 생성하는 내장 지능을 제공합니다. 이를 통해 사용자는 더 큰 데이터 그룹에서 특정 특성을 분리하여 해당 특성이 모델의 부정확한 결과에 주요 영향을 미치는지 확인할 수 있습니다.

![RAI 대시보드의 특성 코호트 - 모델 개요](../../../../translated_images/model-overview-feature-cohorts.c5104d575ffd0c80b7ad8ede7703fab6166bfc6f9125dd395dcc4ace2f522f70.ko.png)

모델 개요 구성 요소는 두 가지 유형의 격차 지표를 지원합니다:

**모델 성능의 격차**: 이 지표는 데이터 하위 그룹 간에 선택된 성능 지표 값의 격차(차이)를 계산합니다. 몇 가지 예는 다음과 같습니다:

- 정확도율의 격차
- 오류율의 격차
- 정밀도의 격차
- 재현율의 격차
- 평균 절대 오차(MAE)의 격차

**선택률의 격차**: 이 지표는 하위 그룹 간의 선택률(유리한 예측)의 차이를 포함합니다. 예를 들어, 대출 승인율의 격차가 이에 해당합니다. 선택률은 각 클래스에서 1로 분류된 데이터 포인트의 비율(이진 분류의 경우) 또는 예측 값의 분포(회귀의 경우)를 의미합니다.

## 데이터 분석

> "데이터를 충분히 괴롭히면, 원하는 답을 얻을 수 있다" - 로널드 코스

이 말은 극단적으로 들릴 수 있지만, 데이터는 어떤 결론이든 지지하도록 조작될 수 있다는 점에서 사실입니다. 이러한 조작은 때로는 의도치 않게 발생하기도 합니다. 인간으로서 우리는 모두 편견을 가지고 있으며, 데이터를 다룰 때 자신이 편견을 도입하고 있는지 의식적으로 알기 어려운 경우가 많습니다. AI와 머신러닝에서 공정성을 보장하는 것은 여전히 복잡한 도전 과제입니다.

데이터는 전통적인 모델 성능 지표에서 큰 블라인드 스팟으로 작용합니다. 높은 정확도 점수를 가지고 있더라도, 이는 데이터셋에 내재된 편향을 항상 반영하지는 않습니다. 예를 들어, 한 회사의 임원직 데이터셋에 여성이 27%, 남성이 73%로 구성되어 있다면, 이 데이터를 기반으로 훈련된 구인 광고 AI 모델은 주로 남성 대상에게 고위직 채용 광고를 타겟팅할 가능성이 높습니다. 데이터의 이러한 불균형은 모델의 예측을 특정 성별에 유리하게 왜곡시켰습니다. 이는 AI 모델에 성별 편향이 존재하는 공정성 문제를 드러냅니다.

RAI 대시보드의 데이터 분석 구성 요소는 데이터셋에서 과대 또는 과소 대표되는 영역을 식별하는 데 도움을 줍니다. 이를 통해 데이터 불균형이나 특정 데이터 그룹의 부족으로 인해 발생하는 오류와 공정성 문제의 근본 원인을 진단할 수 있습니다. 사용자는 예측 결과와 실제 결과, 오류 그룹, 특정 특성을 기반으로 데이터셋을 시각화할 수 있습니다. 때로는 과소 대표된 데이터 그룹을 발견함으로써 모델이 제대로 학습하지 못하고 있다는 사실을 알게 될 수도 있습니다. 데이터 편향이 있는 모델은 공정성 문제일 뿐만 아니라, 모델이 포괄적이거나 신뢰할 수 없음을 나타냅니다.

![RAI 대시보드의 데이터 분석 구성 요소](../../../../translated_images/dataanalysis-cover.8d6d0683a70a5c1e274e5a94b27a71137e3d0a3b707761d7170eb340dd07f11d.ko.png)

데이터 분석을 사용해야 할 때:

- 다양한 필터를 선택하여 데이터를 여러 차원(코호트)으로 나누어 데이터셋 통계를 탐색하고자 할 때.
- 데이터셋이 다양한 코호트와 특성 그룹에 걸쳐 어떻게 분포되어 있는지 이해하고자 할 때.
- 공정성, 오류 분석, 인과 관계(다른 대시보드 구성 요소에서 도출된)와 관련된 발견이 데이터셋의 분포로 인한 것인지 확인하고자 할 때.
- 대표성 문제, 레이블 노이즈, 특성 노이즈, 레이블 편향 등으로 인한 오류를 완화하기 위해 추가 데이터를 수집할 영역을 결정하고자 할 때.

## 모델 해석 가능성

머신러닝 모델은 블랙박스로 간주되는 경우가 많습니다. 모델의 예측을 이끄는 주요 데이터 특성을 이해하는 것은 어려울 수 있습니다. 모델이 특정 예측을 내리는 이유를 투명하게 제공하는 것은 중요합니다. 예를 들어, AI 시스템이 당뇨병 환자가 30일 이내에 병원에 재입원할 위험이 있다고 예측한다면, 해당 예측을 뒷받침하는 데이터를 제공해야 합니다. 이러한 데이터 지표는 투명성을 제공하여 임상 의사나 병원이 잘-informed된 결정을 내릴 수 있도록 돕습니다. 또한, 개별 환자에 대한 모델의 예측 이유를 설명할 수 있는 것은 건강 규제와 관련된 책임성을 보장합니다. 사람들의 삶에 영향을 미치는 방식으로 머신러닝 모델을 사용할 때, 모델의 행동에 영향을 미치는 요인을 이해하고 설명하는 것이 중요합니다. 모델 해석 가능성과 설명 가능성은 다음과 같은 시나리오에서 질문에 답하는 데 도움을 줍니다:

- 모델 디버깅: 내 모델이 왜 이 오류를 발생시켰는가? 모델을 어떻게 개선할 수 있을까?
- 인간-AI 협업: 모델의 결정을 어떻게 이해하고 신뢰할 수 있을까?
- 규제 준수: 내 모델이 법적 요구 사항을 충족하는가?

RAI 대시보드의 특성 중요도 구성 요소는 모델이 예측을 내리는 방식을 디버깅하고 포괄적으로 이해하는 데 도움을 줍니다. 이는 머신러닝 전문가와 의사결정권자가 모델의 행동에 영향을 미치는 특성을 설명하고 규제 준수를 위해 증거를 제시하는 데 유용한 도구입니다. 사용자는 전역 및 로컬 설명을 탐색하여 모델의 예측에 영향을 미치는 특성을 검증할 수 있습니다. 전역 설명은 모델의 전체 예측에 영향을 미친 주요 특성을 나열합니다. 로컬 설명은 개별 사례에 대해 모델의 예측을 이끈 특성을 보여줍니다. 로컬 설명을 평가하는 기능은 특정 사례를 디버깅하거나 감사하여 모델이 정확하거나 부정확한 예측을 내린 이유를 더 잘 이해하고 해석하는 데 유용합니다.

![RAI 대시보드의 특성 중요도 구성 요소](../../../../translated_images/9-feature-importance.cd3193b4bba3fd4bccd415f566c2437fb3298c4824a3dabbcab15270d783606e.ko.png)

- 전역 설명: 예를 들어, 당뇨병 병원 재입원 모델의 전체 행동에 영향을 미치는 특성은 무엇인가?
- 로컬 설명: 예를 들어, 60세 이상의 당뇨병 환자가 이전에 입원한 기록이 있는 경우, 병원에 30일 이내에 재입원할 것으로 예측된 이유는 무엇인가?

다양한 코호트에서 모델의 성능을 조사하는 디버깅 과정에서, 특성 중요도는 특정 특성이 코호트 전반에서 모델의 예측에 어떤 수준의 영향을 미치는지 보여줍니다. 이는 모델의 부정확한 예측을 유도하는 특성의 영향 수준을 비교할 때 이상치를 드러내는 데 도움을 줍니다. 특성 중요도 구성 요소는 특정 특성 값이 모델의 결과에 긍정적 또는 부정적으로 영향을 미쳤는지 보여줄 수 있습니다. 예를 들어, 모델이 부정확한 예측을 내린 경우, 이 구성 요소는 예측을 유도한 특성 또는 특성 값을 세부적으로 조사하고 식별할 수 있는 기능을 제공합니다. 이러한 세부 수준은 디버깅뿐만 아니라 감사 상황에서 투명성과 책임성을 제공하는 데 유용합니다. 마지막으로, 이 구성 요소는 공정성 문제를 식별하는 데 도움을 줄 수 있습니다. 예를 들어, 민감한 특성(예: 인종 또는 성별)이 모델의 예측에 크게 영향을 미친다면, 이는 모델에 인종 또는 성별 편향이 있을 가능성을 나타낼 수 있습니다.

![특성 중요도](../../../../translated_images/9-features-influence.3ead3d3f68a84029f1e40d3eba82107445d3d3b6975d4682b23d8acc905da6d0.ko.png)

해석 가능성을 사용해야 할 때:

- 모델의 예측이 얼마나 신뢰할 수 있는지, 예측에 가장 중요한 특성이 무엇인지 이해하고자 할 때.
- 모델을 먼저 이해하고, 모델이 건강한 특성을 사용하는지 아니면 단순히 잘못된 상관관계를 사용하는지 식별하여 디버깅을 시작하고자 할 때.
- 모델이 민감한 특성이나 이와 높은 상관관계를 가진 특성을 기반으로 예측을 내리는지 이해하여 잠재적인 불공정성을 발견하고자 할 때.
- 로컬 설명을 생성하여 모델의 결과를 설명하고 사용자 신뢰를 구축하고자 할 때.
- AI 시스템의 규제 감사를 완료하여 모델을 검증하고 모델 결정이 인간에게 미치는 영향을 모니터링하고자 할 때.

## 결론

RAI 대시보드의 모든 구성 요소는 사회에 덜 해롭고 더 신뢰할 수 있는 머신러닝 모델을 구축하는 데 도움을 주는 실용적인 도구입니다. 이는 인권에 대한 위협을 예방하고, 특정 그룹을 생애 기회에서 차별하거나 배제하는 것을 방지하며, 신체적 또는 심리적 피해의 위험을 줄이는 데 기여합니다. 또한, 로컬 설명을 생성하여 모델의 결과를 설명함으로써 모델 결정에 대한 신뢰를 구축하는 데 도움을 줍니다. 잠재적인 해악은 다음과 같이 분류될 수 있습니다:

- **할당**: 예를 들어, 특정 성별이나 인종이 다른 성별이나 인종보다 선호되는 경우.
- **서비스 품질**: 특정 시나리오에 맞춰 데이터를 훈련했지만, 실제 상황이 훨씬 더 복잡한 경우, 이는 성능이 낮은 서비스로 이어질 수 있습니다.
- **고정관념화**: 특정 그룹을 미리 정해진 속성과 연관시키는 경우.
- **비하**: 특정 대상이나 사람을 부당하게 비판하고 낙인찍는 경우.
- **과도하거나 부족한 표현**. 특정 그룹이 특정 직업에서 보이지 않는다는 아이디어는, 그러한 상황을 계속 촉진하는 모든 서비스나 기능이 해를 끼치는 데 기여한다는 것을 의미합니다.

### Azure RAI 대시보드

[Azure RAI 대시보드](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)는 Microsoft를 포함한 주요 학술 기관 및 조직에서 개발한 오픈 소스 도구를 기반으로 구축되었습니다. 이는 데이터 과학자와 AI 개발자가 모델의 동작을 더 잘 이해하고, AI 모델에서 발생할 수 있는 바람직하지 않은 문제를 발견하고 완화하는 데 중요한 역할을 합니다.

- RAI 대시보드 [문서](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)를 확인하여 다양한 구성 요소를 사용하는 방법을 배워보세요.

- Azure Machine Learning에서 더 책임 있는 AI 시나리오를 디버깅하기 위한 RAI 대시보드 [샘플 노트북](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks)을 확인해보세요.

---
## 🚀 도전

통계적 또는 데이터 편향이 처음부터 도입되지 않도록 하기 위해 우리는 다음을 수행해야 합니다:

- 시스템 작업에 참여하는 사람들의 배경과 관점을 다양화하기
- 우리 사회의 다양성을 반영하는 데이터셋에 투자하기
- 편향이 발생했을 때 이를 감지하고 수정하는 더 나은 방법 개발하기

모델 구축 및 사용에서 불공정함이 명백한 실제 시나리오를 생각해보세요. 우리가 또 무엇을 고려해야 할까요?

## [강의 후 퀴즈](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)
## 복습 및 자기 학습

이 강의에서는 머신 러닝에서 책임 있는 AI를 통합하는 몇 가지 실용적인 도구를 배웠습니다.

이 워크숍을 시청하여 주제를 더 깊이 탐구하세요:

- 책임 있는 AI 대시보드: 실무에서 RAI를 운영화하기 위한 원스톱 솔루션, Besmira Nushi와 Mehrnoosh Sameki

[![책임 있는 AI 대시보드: 실무에서 RAI를 운영화하기 위한 원스톱 솔루션](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "책임 있는 AI 대시보드: 실무에서 RAI를 운영화하기 위한 원스톱 솔루션")

> 🎥 위 이미지를 클릭하면 Besmira Nushi와 Mehrnoosh Sameki의 "책임 있는 AI 대시보드: 실무에서 RAI를 운영화하기 위한 원스톱 솔루션" 비디오를 볼 수 있습니다.

책임 있는 AI와 더 신뢰할 수 있는 모델을 구축하는 방법에 대해 더 알아보려면 다음 자료를 참조하세요:

- ML 모델 디버깅을 위한 Microsoft의 RAI 대시보드 도구: [책임 있는 AI 도구 리소스](https://aka.ms/rai-dashboard)

- 책임 있는 AI 툴킷 탐색: [Github](https://github.com/microsoft/responsible-ai-toolbox)

- Microsoft의 RAI 리소스 센터: [책임 있는 AI 리소스 – Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Microsoft의 FATE 연구 그룹: [FATE: 공정성, 책임성, 투명성, AI 윤리 - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

## 과제

[RAI 대시보드 탐색](assignment.md)

---

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서를 해당 언어로 작성된 상태에서 권위 있는 자료로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 책임을 지지 않습니다.